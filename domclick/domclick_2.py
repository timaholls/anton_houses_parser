#!/usr/bin/env python3
"""
–ü—Ä–æ—Ö–æ–¥ –ø–æ —Å–ø–∏—Å–∫—É URL –∏ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ API Domclick.

–õ–æ–≥–∏–∫–∞:
- –ß–∏—Ç–∞–µ—Ç JSON —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ (complex_links.json –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
- –î–ª—è –∫–∞–∂–¥–æ–≥–æ URL –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –¥–µ–ª–∞–µ—Ç fetch –∑–∞–ø—Ä–æ—Å—ã –∫ API bff-search-web.domclick.ru/api/offers/v1
- –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º offset (—à–∞–≥ 20): 0, 20, 40, ...
- –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã API: –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∫–≤–∞—Ä—Ç–∏—Ä, –∞–¥—Ä–µ—Å, –Ω–∞–∑–≤–∞–Ω–∏–µ/—Å—Å—ã–ª–∫—É –ñ–ö
- –°–∫–∞—á–∏–≤–∞–µ—Ç –í–°–ï —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ (–ñ–ö + –∫–≤–∞—Ä—Ç–∏—Ä), –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–µ—Ä–µ–∑ resize_img.py (—Å–∂–∞—Ç–∏–µ, –æ—á–∏—Å—Ç–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ S3 –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—É—Ç–∏ –≤ MongoDB:
  - development.photos - –ø—É—Ç–∏ –∫ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º –ñ–ö
  - apartment_types.*.apartments.*.photos - –ø—É—Ç–∏ –∫ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º –∫–≤–∞—Ä—Ç–∏—Ä
- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∏—à–µ—Ç –≤ MongoDB –∏ offers_data.json (–º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤)
- –ü—Ä–æ–≥—Ä–µ—Å—Å —Ö—Ä–∞–Ω–∏—Ç –≤ progress_domclick_2.json: {"url_index": i, "offset": n}
- –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö –¥–µ–ª–∞–µ—Ç –¥–æ 3 –ø–æ–ø—ã—Ç–æ–∫; –ø–æ—Å–ª–µ 3-–π ‚Äî –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ—Ç –±—Ä–∞—É–∑–µ—Ä (–Ω–æ–≤—ã–π –ø—Ä–æ–∫—Å–∏)
  –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Å —Ç–æ–≥–æ –∂–µ –º–µ—Å—Ç–∞
"""
import asyncio
import json
import os
import base64
import logging
from typing import List, Dict, Any, Tuple
from pathlib import Path
import aiohttp
from io import BytesIO
import sys
import shutil
from urllib.parse import urlparse, parse_qs, urlencode

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ç–µ–∫—É—â–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
PROJECT_ROOT = Path(__file__).resolve().parent

# –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
UPLOADS_DIR = PROJECT_ROOT / "uploads"

from browser_manager import create_browser, create_browser_page, restart_browser
from db_manager import save_to_mongodb
from resize_img import ImageProcessor
from s3_service import S3Service
from watermark_on_save import upload_with_watermark

LINKS_FILE = PROJECT_ROOT / "complex_links.json"
PROGRESS_FILE = PROJECT_ROOT / "progress_domclick_2.json"
OUTPUT_FILE = PROJECT_ROOT / "offers_data.json"  # –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π, –æ—Å—Ç–∞–≤–∏–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
START_PAUSE_SECONDS = 5  # –ø–∞—É–∑–∞ –ø–æ—Å–ª–µ –æ—Ç–∫—Ä—ã—Ç–∏—è URL
STEP_PAUSE_SECONDS = 5  # –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏/—à–∞–≥–∞–º–∏

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è ImageProcessor
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –≤—ã–≤–æ–¥–∞ –≤ –∫–æ–Ω—Å–æ–ª—å, –µ—Å–ª–∏ –µ–≥–æ –µ—â–µ –Ω–µ—Ç
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–æ–≤ —á–µ—Ä–µ–∑ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ –ª–æ–≥–≥–µ—Ä—ã
    logger.propagate = False

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
image_processor = ImageProcessor(logger, max_size=(800, 600), max_kb=150)


def create_complex_directory(complex_id: str) -> Path:
    """
    –°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–∞.
    """
    complex_dir = UPLOADS_DIR / "complexes" / complex_id
    complex_photos_dir = complex_dir / "complex_photos"
    apartments_dir = complex_dir / "apartments"

    # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–ø–∫–∏
    complex_photos_dir.mkdir(parents=True, exist_ok=True)
    apartments_dir.mkdir(parents=True, exist_ok=True)

    return complex_dir


def get_complex_id_from_url(url: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç ID –∫–æ–º–ø–ª–µ–∫—Å–∞ –∏–∑ URL.
    """
    try:
        # –ü—Ä–∏–º–µ—Ä: https://domclick.ru/complexes/zhk-8-marta__109690
        parsed = urlparse(url)
        path_parts = parsed.path.split('/')
        if 'complexes' in path_parts:
            complex_index = path_parts.index('complexes')
            if complex_index + 1 < len(path_parts):
                return path_parts[complex_index + 1]
    except Exception:
        pass

    # Fallback - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ö–µ—à URL
    import hashlib
    return hashlib.md5(url.encode()).hexdigest()[:10]


def normalize_complex_url(url: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL –∫–æ–º–ø–ª–µ–∫—Å–∞, –ø—Ä–∏–≤–æ–¥—è –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É.
    –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ufa.domclick.ru –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è.
    """
    if not url:
        return url
    
    try:
        parsed = urlparse(url)
        path_parts = parsed.path.split('/')
        if 'complexes' in path_parts:
            complex_index = path_parts.index('complexes')
            if complex_index + 1 < len(path_parts):
                slug = path_parts[complex_index + 1]
                # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º ufa.domclick.ru
                return f"https://ufa.domclick.ru/complexes/{slug}"
    except Exception:
        pass
    
    return url


async def extract_construction_from_domclick(page, hod_url: str) -> Dict[str, Any]:
    """–ü–µ—Ä–µ—Ö–æ–¥–∏—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ Domclick –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—ã –∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ—Ç–æ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç { construction_stages: [{stage_number, date, photos: [urls<=5]}] }.
    """
    print(f"    üîç –ù–∞—á–∏–Ω–∞—é –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ —Å URL: {hod_url}")
    script = """
    async (targetUrl) => {
      try {
        // –ù–∞–≤–∏–≥–∞—Ü–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É "–•–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞"
        if (location.href !== targetUrl) {
          history.scrollRestoration = 'manual';
        }
      } catch (e) {}
      return null;
    }
    """
    try:
        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞
        print(f"    üìç –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É: {hod_url}")
        await page.goto(hod_url, timeout=120000, waitUntil='networkidle0')
        await asyncio.sleep(3)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å
        page_title = await page.evaluate("() => document.title")
        page_url = await page.evaluate("() => location.href")
        print(f"    üìÑ –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_title}")
        print(f"    üîó –¢–µ–∫—É—â–∏–π URL: {page_url}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        page_info = await page.evaluate("""
        () => {
          const pagination = document.querySelector('[data-testid="construction-progress-pagination"]');
          const images = document.querySelectorAll('img');
          const stages = document.querySelectorAll('[role="listitem"], .stage, [class*="stage"]');
          return {
            hasPagination: !!pagination,
            imagesCount: images.length,
            stagesCount: stages.length,
            bodyText: document.body ? document.body.innerText.substring(0, 200) : ''
          };
        }
        """)
        print(f"    üîç –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: –ø–∞–≥–∏–Ω–∞—Ü–∏—è={page_info.get('hasPagination')}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π={page_info.get('imagesCount')}, —ç—Ç–∞–ø–æ–≤={page_info.get('stagesCount')}")
        print(f"    üìù –ù–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_info.get('bodyText', '')[:100]}...")

        # –ö–ª–∏–∫ –ø–æ –±–µ–π–¥–∂—É –∏ –ø–æ —á–µ–∫–±–æ–∫—Å—É "2025" –≤ –û–î–ù–û–ú evaluate (—Å –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏)
        try:
            clicked_2025 = await page.evaluate(r"""
            async () => {
              const sleep = (ms) => new Promise(r => setTimeout(r, ms));
              // 1) –ö–ª–∏–∫ –ø–æ –±–µ–π–¥–∂—É
              const badge = document.querySelector('[data-badge="true"]');
              if (badge) { badge.click(); await sleep(300); }

              // 2) –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ü–∏—é 2025
              const normalize = (s) => String(s || '').replace(/\s+/g, ' ').trim();
              const options = Array.from(document.querySelectorAll('[role="option"], [aria-selected]'));
              const opt2025 = options.find(el => /\b2025\b/.test(normalize(el.textContent)));
              if (!opt2025) return false;

              // 3) –ò—â–µ–º –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
              const checkbox = opt2025.querySelector('input[type="checkbox"]');
              const target = checkbox || opt2025.querySelector('label, [role="checkbox"], .checkbox-root, .list-cell-root, span[tabindex], div[tabindex]') || opt2025;

              // 4) –≠–º—É–ª—è—Ü–∏—è –∫–ª–∏–∫–∞
              const fire = (type, el) => el && el.dispatchEvent(new MouseEvent(type, { bubbles: true, cancelable: true, view: window }));
              await sleep(150); fire('pointerover', target);
              await sleep(150); fire('mouseover',  target);
              await sleep(180); fire('pointerdown', target);
              await sleep(150); fire('mousedown',   target);
              await sleep(220); fire('pointerup',   target);
              await sleep(180); fire('mouseup',     target);
              await sleep(220);
              return target.dispatchEvent(new MouseEvent('click', { bubbles: true, cancelable: true, view: window }));
            }
            """)
            if clicked_2025:
                await asyncio.sleep(1200/1000)
        except Exception:
            pass

        try:
            await page.waitForSelector('[data-testid="construction-progress-pagination"]', {"timeout": 4000})
        except Exception:
            pass
    except Exception:
        return {"construction_stages": []}

    eval_script = r"""
    () => {
      const toAbs = (u) => { try { return new URL(u, location.origin).href; } catch { return u || null; } };
      const isImg = (u) => /\.(png|jpe?g|webp)(?:$|\?|#)/i.test(String(u || ''));
      const pickFromSrcset = (srcset) => {
        if (!srcset) return null;
        const first = String(srcset).split(',')[0].trim().split(' ')[0];
        return first || null;
      };
      const headerLike = (txt) => {
        if (!txt) return false;
        const s = txt.replace(/\s+/g, ' ').trim();
        if (s.length < 5 || s.length > 160) return false;
        const hasMarkers = /(–∫–≤–∞—Ä—Ç–∞–ª|–∫–≤\.|–ª–∏—Ç–µ—Ä|–æ–±–Ω–æ–≤–ª–µ–Ω|–æ–±–Ω–æ–≤–ª–µ–Ω–æ|–≥–æ–¥|–º–µ—Å—è—Ü)/i.test(s);
        const hasYear = /\b20\d{2}\b/.test(s);
        const hasMonthYear = /[–ê-–Ø–Å][–∞-—è—ë]+,?\s*\d{4}/.test(s);
        return hasMarkers || hasYear || hasMonthYear;
      };
      const collectImages = (root) => {
        const urls = new Set();
        root.querySelectorAll('img').forEach(img => {
          const s1 = img.getAttribute('src');
          const s2 = img.getAttribute('data-src') || img.getAttribute('data-lazy') || img.getAttribute('data-original');
          const s3 = pickFromSrcset(img.getAttribute('srcset'));
          [s1, s2, s3].filter(Boolean).map(toAbs).filter(isImg).forEach(u => urls.add(u));
        });
        root.querySelectorAll('source[srcset]').forEach(s => {
          const picked = pickFromSrcset(s.getAttribute('srcset'));
          if (picked && isImg(picked)) urls.add(toAbs(picked));
        });
        root.querySelectorAll('[style*=\"background\"]').forEach(el => {
          const st = String(el.getAttribute('style') || '');
          const m = st.match(/url\((['\"]?)(.*?)\1\)/i);
          if (m && isImg(m[2])) urls.add(toAbs(m[2]));
        });
        return [...urls];
      };

      const pagination = document.querySelector('[data-testid=\"construction-progress-pagination\"]');
      const container = pagination ? pagination.parentElement : null;
      let upperBlocks = [];
      if (container && pagination) {
        let el = pagination.previousElementSibling;
        while (el) { upperBlocks.push(el); el = el.previousElementSibling; }
        upperBlocks.reverse();
      }
      if (!upperBlocks.length) {
        const candidate = document.querySelector('[role=\"list\"] [role=\"listitem\"]')
          ? document.querySelector('[role=\"list\"]').parentElement
          : document.body;
        upperBlocks = [candidate];
      }

      const seen = new Set();
      const stages = [];

      const extractStageFromBlock = (block) => {
        const headerEl = Array.from(block.querySelectorAll('div,span,p,h1,h2,h3,h4'))
          .find(x => headerLike((x.innerText || '').replace(/\s+/g, ' ').trim()));
        const title = headerEl ? (headerEl.innerText || '').replace(/\s+/g, ' ').trim() : null;
        const photos = collectImages(block).slice(0, 5);
        if (!(title || photos.length)) return;
        const key = `${title || ''}::${photos[0] || ''}`;
        if (!seen.has(key)) {
          stages.push({ title: title || '–≠—Ç–∞–ø', photos });
          seen.add(key);
        }
      };

      upperBlocks.forEach(extractStageFromBlock);
      const filtered = stages.filter(s => s.photos && s.photos.length);
      return filtered;
    }
    """
    # –°–±–æ—Ä —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    stages_merged: List[Dict[str, Any]] = []
    used_keys = set()

    def merge_pages(stages_page: List[Dict[str, Any]]):
        for s in stages_page or []:
            title = s.get('title') or s.get('date') or ''
            photos = list(s.get('photos') or [])[:5]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–º–∏ 5
            key = f"{title}::{photos[0] if photos else ''}"
            if key in used_keys:
                continue
            used_keys.add(key)
            stages_merged.append({
                'stage_number': len(stages_merged) + 1,
                'date': title,
                'photos': photos
            })

    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
        pages_count = await page.evaluate("""
        () => {
          const pag = document.querySelector('[data-testid="construction-progress-pagination"]');
          if (!pag) return 1;
          const nums = Array.from(pag.querySelectorAll('button, a'))
            .map(el => parseInt((el.textContent || '').trim(), 10))
            .filter(n => Number.isFinite(n));
          return Math.max(1, ...(nums.length ? nums : [1]));
        }
        """)
        if not isinstance(pages_count, (int, float)) or pages_count < 1:
            pages_count = 1

        print(f"    üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {pages_count}")
        
        for page_index in range(1, int(pages_count) + 1):
            try:
                print(f"    üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_index}/{pages_count}...")
                data = await page.evaluate(eval_script)
                print(f"    üì¶ –î–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_index}: —Ç–∏–ø={type(data)}, –¥–ª–∏–Ω–∞={len(data) if isinstance(data, (list, dict)) else 'N/A'}")
                
                if isinstance(data, list):
                    print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ —ç—Ç–∞–ø–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_index}: {len(data)}")
                    merge_pages(data)
                elif isinstance(data, dict):
                    stages_list = data.get('stages') or data.get('construction_stages') or []
                    print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ —ç—Ç–∞–ø–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_index}: {len(stages_list)}")
                    merge_pages(stages_list)
                else:
                    print(f"    ‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_index}: {type(data)}")
            except Exception as e:
                print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_index}: {e}")
                import traceback
                traceback.print_exc()

            # –ö–ª–∏–∫–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É, –µ—Å–ª–∏ –µ—Å—Ç—å
            if page_index < pages_count:
                try:
                    clicked = await page.evaluate("""
                    (n) => {
                      const pag = document.querySelector('[data-testid="construction-progress-pagination"]');
                      if (!pag) return false;
                      const btn = Array.from(pag.querySelectorAll('button, a'))
                        .find(el => (el.textContent || '').trim() === String(n));
                      if (btn) { btn.click(); return true; }
                      return false;
                    }
                    """, page_index + 1)
                    if clicked:
                        await asyncio.sleep(2)
                except Exception:
                    pass

        print(f"    ‚úÖ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —ç—Ç–∞–ø–æ–≤: {len(stages_merged)}")
        if stages_merged:
            print(f"    üì∏ –ü—Ä–∏–º–µ—Ä—ã —Ñ–æ—Ç–æ –∏–∑ —ç—Ç–∞–ø–æ–≤:")
            for idx, stage in enumerate(stages_merged[:3], 1):
                photos_count = len(stage.get('photos', []))
                print(f"      –≠—Ç–∞–ø {idx}: –¥–∞—Ç–∞={stage.get('date', 'N/A')}, —Ñ–æ—Ç–æ={photos_count}")
        
        return {"construction_stages": stages_merged}
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞: {e}")
        import traceback
        traceback.print_exc()
        return {"construction_stages": []}


async def process_construction_stages_domclick(stages: List[Dict[str, Any]], complex_id: str) -> Dict[str, Any]:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–æ—Ç–æ –ø–æ —ç—Ç–∞–ø–∞–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤ S3, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É construction_progress —Å URL."""
    if not stages:
        return {"construction_stages": []}
    s3 = S3Service()
    result_stages = []
    async with aiohttp.ClientSession() as session:
        for s in stages:
            stage_num = s.get("stage_number") or (len(result_stages) + 1)
            urls = (s.get("photos") or [])[:5]  # —Å–∫–∞—á–∏–≤–∞–µ–º –Ω–µ –±–æ–ª–µ–µ 5 —Ñ–æ—Ç–æ –Ω–∞ —ç—Ç–∞–ø
            saved = []
            sem = asyncio.Semaphore(5)
            async def work(u, idx):
                async with sem:
                    try:
                        async with session.get(u, timeout=aiohttp.ClientTimeout(total=30)) as response:
                            if response.status != 200:
                                return None
                            raw = await response.read()
                    except Exception:
                        return None
                    input_bytes = BytesIO(raw)
                    try:
                        processed = image_processor.process(input_bytes)
                    except Exception:
                        return None
                    processed.seek(0)
                    data = processed.read()
                    key = f"complexes/{complex_id}/construction/stage_{stage_num}/photo_{idx + 1}.jpg"
                    try:
                        return upload_with_watermark(s3, data, key)
                    except Exception:
                        return None
            tasks = [work(u, i) for i, u in enumerate(urls)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for p in results:
                if isinstance(p, str) and p:
                    saved.append(p)
            result_stages.append({
                "stage_number": stage_num,
                "date": s.get("date") or "",
                "photos": saved
            })
    return {"construction_stages": result_stages}


def save_processed_image(image_data: bytes, file_path: Path) -> bool:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª.
    """
    try:
        file_path.parent.mkdir(parents=True, exist_ok=True)
        with open(file_path, 'wb') as f:
            f.write(image_data)
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return False


def load_links(path: str = str(LINKS_FILE)) -> List[str]:
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    # –¥–æ–ø—É—Å–∫–∞–µ–º –∫–∞–∫ —Å–ø–∏—Å–æ–∫, —Ç–∞–∫ –∏ —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º links
    if isinstance(data, dict) and "links" in data:
        return list(data.get("links") or [])
    return list(data or [])


def load_progress(path: str = str(PROGRESS_FILE)) -> Tuple[int, int]:
    if not os.path.exists(path):
        return 0, 0
    try:
        with open(path, "r", encoding="utf-8") as f:
            obj = json.load(f)
        return int(obj.get("url_index", 0)), int(obj.get("offset", 0))
    except Exception:
        return 0, 0


def save_progress(url_index: int, offset: int, path: str = str(PROGRESS_FILE)) -> None:
    tmp_path = path + ".tmp"
    with open(tmp_path, "w", encoding="utf-8") as f:
        json.dump({"url_index": url_index, "offset": offset}, f, ensure_ascii=False)
    os.replace(tmp_path, path)


def extract_url_params(url: str) -> Dict[str, Any]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ URL –ø–æ–∏—Å–∫–∞ Domclick –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è API –∑–∞–ø—Ä–æ—Å–∞.
    """
    try:
        parsed = urlparse(url)
        params = parse_qs(parsed.query, keep_blank_values=True)
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–∫–∏ –≤ —Å—Ç—Ä–æ–∫–∏ (–±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
        result = {}
        for key, value_list in params.items():
            if value_list:
                result[key] = value_list[0] if len(value_list) == 1 else value_list
        return result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ URL {url}: {e}")
        return {}


async def fetch_offers_api(page, api_params: Dict[str, Any], offset: int, max_retries: int = 3) -> Dict[str, Any]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç fetch –∑–∞–ø—Ä–æ—Å –∫ API Domclick —á–µ—Ä–µ–∑ page.evaluate().
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç API –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ.
    –ü–æ–≤—Ç–æ—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–æ max_retries —Ä–∞–∑ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö.
    """
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è API –∑–∞–ø—Ä–æ—Å–∞
    api_params_copy = api_params.copy()
    api_params_copy['offset'] = str(offset)
    api_params_copy['limit'] = api_params_copy.get('limit', '20')
    api_params_copy.setdefault('sort', 'price')
    api_params_copy.setdefault('sort_dir', 'desc')
    api_params_copy.setdefault('deal_type', 'sale')
    api_params_copy.setdefault('category', 'living')
    api_params_copy.setdefault('offer_type', 'layout')
    api_params_copy.setdefault('from_developer', '1')
    api_params_copy.setdefault('disable_payment', 'true')
    api_params_copy.setdefault('enable_mixed_ranking', '1')
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º query string —Å –ø–æ–º–æ—â—å—é urlencode
    query_string = urlencode(api_params_copy, doseq=True)
    api_url = f"https://bff-search-web.domclick.ru/api/offers/v1?{query_string}"

    script = """
    async (url) => {
      try {
        const response = await fetch(url, {
          headers: {
            'accept': 'application/json, text/plain, */*',
            'accept-language': 'ru,en;q=0.9',
            'sec-ch-ua': '"Not A(Brand";v="8", "Chromium";v="132", "YaBrowser";v="25.2", "Yowser";v="2.5"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Linux"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-site'
          },
          referrer: 'https://ufa.domclick.ru/',
          referrerPolicy: 'strict-origin-when-cross-origin',
          method: 'GET',
          mode: 'cors',
          credentials: 'include'
        });
        
        if (!response.ok) {
          return { error: 'HTTP ' + response.status + ': ' + response.statusText };
        }
        
        const data = await response.json();
        return data;
      } catch (error) {
        return { error: error.toString() };
      }
    }
    """
    
    for attempt in range(1, max_retries + 1):
        try:
            print(api_url)

            result = await page.evaluate(script, api_url)
            if isinstance(result, dict):
                if 'error' in result:
                    logger.warning(f"API –∑–∞–ø—Ä–æ—Å offset={offset} –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}): {result['error']}")
                    if attempt < max_retries:
                        await asyncio.sleep(2 * attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: 2, 4, 6 —Å–µ–∫—É–Ω–¥
                        continue
                    return None
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                logger.info(f"API –æ—Ç–≤–µ—Ç offset={offset}: –∫–ª—é—á–∏ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è: {list(result.keys())}")
                if 'result' in result:
                    logger.info(f"  –ù–∞–π–¥–µ–Ω –∫–ª—é—á 'result', –µ–≥–æ –∫–ª—é—á–∏: {list(result['result'].keys()) if isinstance(result['result'], dict) else '–Ω–µ —Å–ª–æ–≤–∞—Ä—å'}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ–±–µ—Ä—Ç–∫–∞ 'result'
                if 'result' in result and isinstance(result['result'], dict):
                    # –î–∞–Ω–Ω—ã–µ –æ–±–µ—Ä–Ω—É—Ç—ã –≤ 'result'
                    actual_data = result['result']
                    logger.info(f"  –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ result, –∫–ª—é—á–∏: {list(actual_data.keys())}")
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ total –∏–∑ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è, –µ—Å–ª–∏ –æ–Ω —Ç–∞–º –µ—Å—Ç—å
                    if 'total' in result:
                        actual_data['total'] = result['total']
                        logger.info(f"  –ù–∞–π–¥–µ–Ω total –≤ –≤–µ—Ä—Ö–Ω–µ–º —É—Ä–æ–≤–Ω–µ: {result['total']}")
                    return actual_data
                
                # –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –±–µ–∑ –æ–±–µ—Ä—Ç–∫–∏
                logger.info(f"  –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é, –∫–ª—é—á–∏: {list(result.keys())}")
                if 'total' in result:
                    logger.info(f"  –ù–∞–π–¥–µ–Ω total: {result['total']}")
                return result
            else:
                logger.warning(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ API offset={offset} (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries})")
                if attempt < max_retries:
                    await asyncio.sleep(2 * attempt)
                    continue
                return None
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è fetch –∑–∞–ø—Ä–æ—Å–∞ offset={offset} (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}): {e}")
            if attempt < max_retries:
                await asyncio.sleep(2 * attempt)
                continue
            return None
    
    return None


async def download_and_process_image(session: aiohttp.ClientSession, image_url: str, file_path: Path) -> str:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ URL, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –µ–≥–æ —á–µ—Ä–µ–∑ resize_img.py –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É.
    """
    try:
        async with session.get(image_url, timeout=aiohttp.ClientTimeout(total=30)) as response:
            if response.status == 200:
                image_bytes = await response.read()

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ resize_img.py
                input_bytes = BytesIO(image_bytes)
                try:
                    processed_bytes = image_processor.process(input_bytes)
                except Exception as process_error:
                    logger.error(f"–û—à–∏–±–∫–∞ resize_img.py: {process_error}")
                    return None

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
                processed_bytes.seek(0)
                image_data = processed_bytes.read()

                if save_processed_image(image_data, file_path):
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç uploads
                    relative_path = file_path.relative_to(UPLOADS_DIR)
                    return str(relative_path).replace('\\', '/')  # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
                else:
                    return None
            else:
                logger.warning(f"HTTP {response.status} –¥–ª—è {image_url}")
                return None
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {image_url}: {e}")
        return None


async def process_complex_photos(photo_urls: List[str], complex_id: str) -> List[str]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ñ–ö –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Ö –≤ S3.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—É–±–ª–∏—á–Ω—ã—Ö URL.
    """
    if not photo_urls:
        return []

    processed_photos = []
    s3 = S3Service()

    async with aiohttp.ClientSession() as session:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ 5 —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        semaphore = asyncio.Semaphore(5)

        async def process_single_photo(url, index):
            async with semaphore:
                # –°–∫–∞—á–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω–∏–∫
                try:
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                        if response.status != 200:
                            return None
                        raw = await response.read()
                except Exception:
                    return None

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ resize
                input_bytes = BytesIO(raw)
                try:
                    processed = image_processor.process(input_bytes)
                except Exception:
                    return None
                processed.seek(0)
                data = processed.read()

                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ S3
                key = f"complexes/{complex_id}/complex_photos/photo_{index + 1}.jpg"
                try:
                    url_public = upload_with_watermark(s3, data, key)
                    return url_public
                except Exception:
                    return None

        tasks = [process_single_photo(url, i) for i, url in enumerate(photo_urls[:8])]  # –º–∞–∫—Å–∏–º—É–º 8 —Ñ–æ—Ç–æ –ñ–ö
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, str) and result:
                processed_photos.append(result)

    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(processed_photos)} –∏–∑ {len(photo_urls)} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ñ–ö")
    return processed_photos


async def process_apartment_photos(apartment_data: Dict[str, Any], complex_id: str, apartment_path: str) -> Dict[str, Any]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –¥–ª—è –æ–¥–Ω–æ–π –∫–≤–∞—Ä—Ç–∏—Ä—ã –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤ S3.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å URL –∫ —Ñ–∞–π–ª–∞–º.
    """
    if not apartment_data.get("images"):
        return {
            "offer": apartment_data.get("offer"),
            "photos": []
        }

    image_urls = apartment_data["images"]
    if not image_urls:
        return {
            "offer": apartment_data.get("offer"),
            "photos": []
        }

    processed_images = []
    s3 = S3Service()

    async with aiohttp.ClientSession() as session:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ 3 —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –∫–≤–∞—Ä—Ç–∏—Ä
        semaphore = asyncio.Semaphore(3)

        async def process_single_photo(url, index):
            async with semaphore:
                try:
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                        if response.status != 200:
                            return None
                        raw = await response.read()
                except Exception:
                    return None

                input_bytes = BytesIO(raw)
                try:
                    processed = image_processor.process(input_bytes)
                except Exception:
                    return None
                processed.seek(0)
                data = processed.read()

                key = f"complexes/{complex_id}/apartments/{apartment_path}/photo_{index + 1}.jpg"
                try:
                    url_public = upload_with_watermark(s3, data, key)
                    return url_public
                except Exception:
                    return None

        tasks = [process_single_photo(url, i) for i, url in enumerate(image_urls[:3])]  # –º–∞–∫—Å–∏–º—É–º 3 —Ñ–æ—Ç–æ –Ω–∞ –∫–≤–∞—Ä—Ç–∏—Ä—É
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for i, result in enumerate(results):
            if isinstance(result, str) and result:
                processed_images.append(result)
            elif isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ {i + 1}: {result}")
            else:
                logger.warning(f"–§–æ—Ç–æ {i + 1} –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {type(result)}")

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–≤–∞—Ä—Ç–∏—Ä—ã —Å URL –∫ —Ñ–∞–π–ª–∞–º
    result = {
        "offer": apartment_data.get("offer"),
        "photos": processed_images
    }
    return result


async def process_all_apartment_types(apartment_types: Dict[str, Any], complex_id: str) -> Dict[str, Any]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –≤–æ –≤—Å–µ—Ö —Ç–∏–ø–∞—Ö –∫–≤–∞—Ä—Ç–∏—Ä –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤ S3.
    """
    if not apartment_types:
        return apartment_types

    processed_types = {}

    for apartment_type, type_data in apartment_types.items():
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        if isinstance(type_data, list):
            # –ï—Å–ª–∏ type_data - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –∫–≤–∞—Ä—Ç–∏—Ä –Ω–∞–ø—Ä—è–º—É—é
            apartments = type_data
        elif isinstance(type_data, dict) and "apartments" in type_data:
            # –ï—Å–ª–∏ type_data - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º "apartments"
            apartments = type_data.get("apartments", [])
        else:
            # –ï—Å–ª–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            processed_types[apartment_type] = type_data
            continue

        processed_apartments = []
        apartment_type_normalized = apartment_type.replace('-', '_').replace('–∫–æ–º–Ω', 'komn')

        for i, apartment in enumerate(apartments):
            if isinstance(apartment, dict):
                apartment_path = f"{apartment_type_normalized}/apartment_{i + 1}"
                processed_apartment = await process_apartment_photos(apartment, complex_id, apartment_path)
                processed_apartments.append(processed_apartment)
            else:
                processed_apartments.append(apartment)

        # –ü—Ä–∞–≤–∏–ª—å–Ω–æ —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        if isinstance(type_data, list):
            # –ï—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —Å–ø–∏—Å–∫–æ–º, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫
            processed_types[apartment_type] = processed_apartments
        else:
            # –ï—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —Å–ª–æ–≤–∞—Ä–µ–º, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
            processed_types[apartment_type] = {
                **type_data,
                "apartments": processed_apartments
            }

    return processed_types


def normalize_room_from_api(rooms: int) -> str:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç –∏–∑ API –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏.
    """
    if rooms == 0:
        return '–°—Ç—É–¥–∏—è'
    return f'{rooms}-–∫–æ–º–Ω'


def process_api_response(api_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç API –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å offers (–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –∫–æ–º–Ω–∞—Ç–∞–º), address, complexName, complexHref.
    """
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ API: –∫–ª—é—á–∏ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è: {list(api_data.keys()) if api_data else 'None'}")
    
    if not api_data:
        logger.warning("  api_data –ø—É—Å—Ç–æ–π –∏–ª–∏ None")
        return {
            'offers': {},
            'address': None,
            'complexName': None,
            'complexHref': None
        }
    
    if 'items' not in api_data:
        logger.warning(f"  –ö–ª—é—á 'items' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ api_data. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {list(api_data.keys())}")
        return {
            'offers': {},
            'address': None,
            'complexName': None,
            'complexHref': None
        }
    
    items = api_data.get('items', [])
    
    if not items:
        return {
            'offers': {},
            'address': None,
            'complexName': None,
            'complexHref': None
        }
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
    first_item = items[0]
    
    address = first_item.get('address', {}).get('displayName')
    
    complex_data = first_item.get('complex', {})
    complex_name = complex_data.get('name')
    complex_slug = complex_data.get('slug')
    complex_id = complex_data.get('id')
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å
    complex_href = None
    if complex_slug:
        complex_href = f"https://ufa.domclick.ru/complexes/{complex_slug}"
    elif complex_id:
        complex_href = f"https://ufa.domclick.ru/complexes/{complex_id}"
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∫–≤–∞—Ä—Ç–∏—Ä—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–æ–º–Ω–∞—Ç
    offers = {}
    skipped_count = 0
    total_items = len(items)
    
    for idx, item in enumerate(items):
        if not isinstance(item, dict):
            logger.warning(f"  ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —ç–ª–µ–º–µ–Ω—Ç {idx+1}/{total_items}: –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º (—Ç–∏–ø: {type(item).__name__})")
            skipped_count += 1
            continue
            
        general_info = item.get('generalInfo', {})
        if not general_info:
            logger.warning(f"  ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —ç–ª–µ–º–µ–Ω—Ç {idx+1}/{total_items}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç generalInfo. –ö–ª—é—á–∏ —ç–ª–µ–º–µ–Ω—Ç–∞: {list(item.keys())[:10]}")
            skipped_count += 1
            continue
            
        rooms = general_info.get('rooms', 0)
        room_key = normalize_room_from_api(rooms)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–≤–∞—Ä—Ç–∏—Ä—ã
        area = general_info.get('area')
        min_floor = general_info.get('minFloor')
        max_floor = general_info.get('maxFloor')
        
        title_parts = []
        if rooms == 0:
            title_parts.append('–°—Ç—É–¥–∏—è')
        else:
            title_parts.append(f'{rooms}-–∫–æ–º–Ω')
        if area:
            title_parts.append(f'{area} –º¬≤')
        if min_floor is not None and max_floor is not None:
            if min_floor == max_floor:
                title_parts.append(f'{min_floor} —ç—Ç–∞–∂')
            else:
                title_parts.append(f'{min_floor}-{max_floor} —ç—Ç–∞–∂')
        
        title = ', '.join(title_parts) if title_parts else '–ö–≤–∞—Ä—Ç–∏—Ä–∞'
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
        photos = item.get('photos', [])
        image_urls = []
        
        for photo_idx, photo in enumerate(photos):
            if not isinstance(photo, dict):
                continue
            photo_url = photo.get('url', '')
            if photo_url:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL: https://img.dmclk.ru/ + –ø—É—Ç—å
                if photo_url.startswith('/'):
                    full_url = f"https://img.dmclk.ru{photo_url}"
                elif photo_url.startswith('http'):
                    full_url = photo_url
                else:
                    full_url = f"https://img.dmclk.ru/{photo_url}"
                image_urls.append(full_url)
        
        card = {
            'offer': title,
            'photos': image_urls  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 'photos' –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å MongoDB —Å—Ö–µ–º–æ–π
        }
        
        if room_key not in offers:
            offers[room_key] = []
        offers[room_key].append(card)
    
    processed_count = sum(len(cards) for cards in offers.values())
    logger.info(f"  –ò—Ç–æ–≥–æ: –ø–æ–ª—É—á–µ–Ω–æ={total_items}, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ={processed_count}, –ø—Ä–æ–ø—É—â–µ–Ω–æ={skipped_count}, –≥—Ä—É–ø–ø={len(offers)}")
    
    if skipped_count > 0:
        logger.warning(f"  ‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ü—Ä–æ–ø—É—â–µ–Ω–æ {skipped_count} –∏–∑ {total_items} —ç–ª–µ–º–µ–Ω—Ç–æ–≤!")
    
    return {
        'offers': offers,
        'address': address,
        'complexName': complex_name,
        'complexHref': complex_href
    }


def log_apartment_photo_parsing(offers: Dict[str, List[Dict[str, Any]]], *, base_url: str, offset: int) -> None:
    """
    –õ–æ–≥–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –∫–≤–∞—Ä—Ç–∏—Ä–∞—Ö (—Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ).
    """
    if not offers:
        return
    
    total_apartments = sum(len(cards) if isinstance(cards, list) else 0 for cards in offers.values())
    total_photos = 0
    for cards in offers.values():
        if isinstance(cards, list):
            for card in cards:
                if isinstance(card, dict):
                    images = card.get("photos") or card.get("images") or []
                    total_photos += len(images)
    
    logger.info(f"  –°–æ–±—Ä–∞–Ω–æ –∫–≤–∞—Ä—Ç–∏—Ä: {total_apartments}, –≥—Ä—É–ø–ø: {len(offers)}, —Ñ–æ—Ç–æ: {total_photos}")


async def run() -> None:
    urls = load_links(str(LINKS_FILE))
    if not urls:
        print("–§–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –ø—É—Å—Ç –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç:", LINKS_FILE)
        return

    url_index, offset = load_progress(str(PROGRESS_FILE))
    url_index = max(0, min(url_index, len(urls)))
    print(f"–°—Ç–∞—Ä—Ç: url_index={url_index}, offset={offset}, –≤—Å–µ–≥–æ URL: {len(urls)}")

    results: List[Dict[str, Any]] = []
    if os.path.exists(str(OUTPUT_FILE)):
        try:
            with open(str(OUTPUT_FILE), 'r', encoding='utf-8') as f:
                old = json.load(f)
                if isinstance(old, list):
                    results = old
        except Exception:
            pass

    # –°–æ–∑–¥–∞–µ–º –±—Ä–∞—É–∑–µ—Ä —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø—Ä–æ–∫—Å–∏
    browser = None
    page = None
    max_init_attempts = 5
    
    for init_attempt in range(max_init_attempts):
        try:
            browser, proxy_url = await create_browser(headless=False)
            print(f"–ü–æ–ø—ã—Ç–∫–∞ {init_attempt + 1}/{max_init_attempts}: –°–æ–∑–¥–∞–Ω –±—Ä–∞—É–∑–µ—Ä —Å –ø—Ä–æ–∫—Å–∏ {proxy_url}")
            page = await create_browser_page(browser)
            print("‚úì –ë—Ä–∞—É–∑–µ—Ä –∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            break
        except Exception as init_error:
            print(f"‚úó –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞ (–ø–æ–ø—ã—Ç–∫–∞ {init_attempt + 1}/{max_init_attempts}): {init_error}")
            if browser:
                try:
                    await browser.close()
                except:
                    pass
            if init_attempt < max_init_attempts - 1:
                await asyncio.sleep(2)
            else:
                print("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
                return

    try:
        while url_index < len(urls):
            base_url = urls[url_index]
            print(f"‚Üí URL [{url_index + 1}/{len(urls)}]: {base_url}")

            if offset % 20 != 0:
                offset = (offset // 20) * 20

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ URL
            api_params = extract_url_params(base_url)
            if not api_params:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ URL: {base_url}. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
                url_index += 1
                offset = 0
                save_progress(url_index, offset, str(PROGRESS_FILE))
                continue

            # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–∑ —Ñ–∞–π–ª–∞ –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ cookies –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±—Ä–∞—É–∑–µ—Ä–∞
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º waitUntil: 'networkidle0' —á—Ç–æ–±—ã –¥–æ–∂–¥–∞—Ç—å—Å—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            try:
                print(f"  –û—Ç–∫—Ä—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {base_url}")
                await page.goto(base_url, timeout=120000, waitUntil='networkidle0')
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∂–¥–µ–º, –ø–æ–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è
                await page.waitForFunction(
                    "() => document.readyState === 'complete'",
                    {"timeout": 30000}
                )
                
                # –ñ–¥–µ–º –µ—â–µ –Ω–µ–º–Ω–æ–≥–æ, —á—Ç–æ–±—ã –≤—Å–µ —Å–∫—Ä–∏–ø—Ç—ã –≤—ã–ø–æ–ª–Ω–∏–ª–∏—Å—å
                await asyncio.sleep(3)
                print(f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∫–æ–Ω—Ç–µ–∫—Å—Ç –≥–æ—Ç–æ–≤")
            except Exception as e:
                print(f"  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É: {e}")
                # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ –æ—Ç–∫—Ä—ã—Ç–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã

            # –î–µ–ª–∞–µ–º –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            attempts = 0
            first_api_response = None
            while attempts < 3:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –µ—â–µ –∂–∏–≤–∞ –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∂–µ–Ω–∞
                    try:
                        ready_state = await page.evaluate("() => document.readyState")
                        if ready_state != 'complete':
                            print(f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (readyState: {ready_state}), –∂–¥—É...")
                            await page.waitForFunction(
                                "() => document.readyState === 'complete'",
                                {"timeout": 30000}
                            )
                            await asyncio.sleep(2)
                    except Exception:
                        # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —É–Ω–∏—á—Ç–æ–∂–µ–Ω, –ø–µ—Ä–µ–æ—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                        print(f"  –ö–æ–Ω—Ç–µ–∫—Å—Ç —É–Ω–∏—á—Ç–æ–∂–µ–Ω, –ø–µ—Ä–µ–æ—Ç–∫—Ä—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
                        await page.goto(base_url, timeout=120000, waitUntil='networkidle0')
                        await page.waitForFunction(
                            "() => document.readyState === 'complete'",
                            {"timeout": 30000}
                        )
                        await asyncio.sleep(3)
                    
                    print(f"  –ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö offset=0...")
                    first_api_response = await fetch_offers_api(page, api_params, 0, max_retries=3)
                    if first_api_response and 'items' in first_api_response:
                        print(f"  ‚úì –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è offset=0")
                        break
                    attempts += 1
                    if attempts < 3:
                        print(f"  –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ 2 —Å–µ–∫—É–Ω–¥—ã...")
                        await asyncio.sleep(2)
                except Exception as e:
                    attempts += 1
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º API –∑–∞–ø—Ä–æ—Å–µ: {e} (–ø–æ–ø—ã—Ç–∫–∞ {attempts}/3)")
                    if attempts >= 3:
                        try:
                            browser, page, _ = await restart_browser(browser, headless=False)
                            attempts = 0
                        except Exception as restart_error:
                            print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ –±—Ä–∞—É–∑–µ—Ä–∞: {restart_error}")
                            break
                    else:
                        await asyncio.sleep(2)

            if not first_api_response:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ API –¥–ª—è URL: {base_url}. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
                url_index += 1
                offset = 0
                save_progress(url_index, offset, str(PROGRESS_FILE))
                continue

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ —Å—Ç—Ä–∞–Ω–∏—Ü
            total = first_api_response.get('total', 0)
            items_count = len(first_api_response.get('items', []))
            limit = int(api_params.get('limit', 20))
            
            logger.info(f"  –û—Ç–≤–µ—Ç API: total={total}, items –≤ –æ—Ç–≤–µ—Ç–µ={items_count}, limit={limit}")
            
            # –ï—Å–ª–∏ total=0, –Ω–æ –µ—Å—Ç—å items, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ items –∫–∞–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä
            if total == 0 and items_count > 0:
                logger.warning(f"  total=0, –Ω–æ –Ω–∞–π–¥–µ–Ω–æ {items_count} items. –ë—É–¥–µ–º –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å –ø–æ–∫–∞ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ.")
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã —Ü–∏–∫–ª —Ä–∞–±–æ—Ç–∞–ª, –Ω–æ –±—É–¥–µ–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
                total = items_count + 1  # –ß—Ç–æ–±—ã —Ü–∏–∫–ª –≤—ã–ø–æ–ª–Ω–∏–ª—Å—è —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ä–∞–∑
            
            total_pages = max(1, (total + limit - 1) // limit) if total > 0 else 1
            print(f"  –í—Å–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {total}, items –≤ –ø–µ—Ä–≤–æ–º –æ—Ç–≤–µ—Ç–µ: {items_count}, —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–π –æ—Ç–≤–µ—Ç
            first_data = process_api_response(first_api_response)
            aggregated_address = first_data.get('address')
            aggregated_complex_name = first_data.get('complexName')
            aggregated_complex_href = first_data.get('complexHref')
            aggregated_offers = first_data.get('offers', {})
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–π batch
            log_apartment_photo_parsing(aggregated_offers, base_url=base_url, offset=0)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            current_offset = limit
            # –ï—Å–ª–∏ total –±—ã–ª —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ (–∏–∑-–∑–∞ total=0), –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥–æ–π –ø–æ–¥—Ö–æ–¥
            if total == items_count + 1:
                # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–∫–∞ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
                while True:
                    print(f"  –ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö offset={current_offset}...")
                    api_response = await fetch_offers_api(page, api_params, current_offset, max_retries=3)
                    
                    if api_response and 'items' in api_response:
                        response_items = api_response.get('items', [])
                        if not response_items:
                            print(f"  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è offset={current_offset}, –∑–∞–≤–µ—Ä—à–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É")
                            break
                        
                        data = process_api_response(api_response)
                        offers = data.get('offers', {})
                        log_apartment_photo_parsing(offers, base_url=base_url, offset=current_offset)
                        
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≥—Ä—É–ø–ø—ã –æ—Ñ—Ñ–µ—Ä–æ–≤
                        for group, cards in offers.items():
                            if group not in aggregated_offers:
                                aggregated_offers[group] = []
                            aggregated_offers[group].extend(cards)
                        
                        offset = current_offset + limit
                        save_progress(url_index, offset, str(PROGRESS_FILE))
                        print(f"  ‚úì –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω–æ {len(response_items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è offset={current_offset}")
                        
                        # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ limit —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                        if len(response_items) < limit:
                            print(f"  –ü–æ–ª—É—á–µ–Ω–æ –º–µ–Ω—å—à–µ limit ({len(response_items)} < {limit}), —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
                            break
                    else:
                        print(f"  ‚úó –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è offset={current_offset}, –∑–∞–≤–µ—Ä—à–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É")
                        break
                    
                    # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(3)  # –ü–∞—É–∑–∞ 3 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    current_offset += limit
            else:
                # –û–±—ã—á–Ω—ã–π —Å–ª—É—á–∞–π: total –∏–∑–≤–µ—Å—Ç–µ–Ω
                while current_offset < total:
                    print(f"  –ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö offset={current_offset}...")
                    api_response = await fetch_offers_api(page, api_params, current_offset, max_retries=3)
                    
                    if api_response and 'items' in api_response:
                        data = process_api_response(api_response)
                        offers = data.get('offers', {})
                        log_apartment_photo_parsing(offers, base_url=base_url, offset=current_offset)
                        
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≥—Ä—É–ø–ø—ã –æ—Ñ—Ñ–µ—Ä–æ–≤
                        for group, cards in offers.items():
                            if group not in aggregated_offers:
                                aggregated_offers[group] = []
                            aggregated_offers[group].extend(cards)
                        
                        offset = current_offset + limit
                        save_progress(url_index, offset, str(PROGRESS_FILE))
                        print(f"  ‚úì –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è offset={current_offset}")
                    else:
                        print(f"  ‚úó –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è offset={current_offset}, –ø—Ä–æ–ø—É—Å–∫–∞—é")
                    
                    # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ)
                    if current_offset + limit < total:
                        await asyncio.sleep(3)  # –ü–∞—É–∑–∞ 3 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    
                    current_offset += limit

            # –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ñ–ö –∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ö–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –Ω—É–∂–Ω–æ –æ—Ç–∫—Ä—ã—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–º–ø–ª–µ–∫—Å–∞
            complex_gallery_images: List[str] = []
            aggregated_hod_url: str = None
            construction_progress_data: Dict[str, Any] = None
            
            if aggregated_complex_href:
                try:
                    print(f"  –û—Ç–∫—Ä—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–º–ø–ª–µ–∫—Å–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ñ–ö: {aggregated_complex_href}")
                    await page.goto(aggregated_complex_href, timeout=120000)
                    await asyncio.sleep(3)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ñ–ö –∏–∑ –≥–∞–ª–µ—Ä–µ–∏
                    try:
                        complex_photos_data = await page.evaluate("""
                        () => {
                          const complexPhotos = [];
                          
                          // –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –≥–∞–ª–µ—Ä–µ–∏
                          let galleryContainer = document.querySelector('[data-e2e-id="complex-header-gallery"]');
                          if (!galleryContainer) {
                            // –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                            galleryContainer = document.querySelector('[data-e2e-id*="gallery"]');
                          }
                          if (!galleryContainer) {
                            // –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –∫–ª–∞—Å—Å—É
                            galleryContainer = document.querySelector('.gallery, [class*="gallery"], [class*="Gallery"]');
                          }
                          
                          console.log('Gallery container found:', !!galleryContainer);
                          
                          if (galleryContainer) {
                            // –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                            let imageElements = galleryContainer.querySelectorAll('[data-e2e-id^="complex-header-gallery-image__"]');
                            if (imageElements.length === 0) {
                              // –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
                              imageElements = galleryContainer.querySelectorAll('img');
                            }
                            
                            console.log('Image elements found:', imageElements.length);
                            
                            imageElements.forEach((element, idx) => {
                              // –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –ø–æ–ª—É—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                              let img = element;
                              if (element.tagName !== 'IMG') {
                                img = element.querySelector('img');
                              }
                              
                              if (!img) {
                                // –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ img –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–∞
                                img = element.querySelector('img.picture-image-object-fit--cover-820-5-0-5.picture-imageFillingContainer-4a2-5-0-5');
                              }
                              if (!img) {
                                // –ü—Ä–æ–±—É–µ–º –ª—é–±–æ–π img
                                img = element.querySelector('img');
                              }
                              
                              if (img) {
                                // –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è URL
                                let imgUrl = img.src || img.getAttribute('src') || img.getAttribute('data-src') || 
                                           img.getAttribute('data-lazy') || img.getAttribute('data-original');
                                
                                if (imgUrl) {
                                  try {
                                    const absoluteUrl = new URL(imgUrl, location.origin).href;
                                    // –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                    if (/\.(jpg|jpeg|png|webp)/i.test(absoluteUrl) || absoluteUrl.includes('img.dmclk.ru') || absoluteUrl.includes('vitrina')) {
                                      complexPhotos.push(absoluteUrl);
                                    }
                                  } catch (e) {
                                    if (imgUrl.startsWith('http')) {
                                      complexPhotos.push(imgUrl);
                                    }
                                  }
                                }
                              }
                            });
                          }
                          
                          console.log('Total photos found:', complexPhotos.length);
                          return complexPhotos;
                        }
                        """)
                        complex_gallery_images = complex_photos_data or []
                        print(f"  –ù–∞–π–¥–µ–Ω–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ñ–ö: {len(complex_gallery_images)}")
                        if complex_gallery_images:
                            print(f"  –ü—Ä–∏–º–µ—Ä—ã URL —Ñ–æ—Ç–æ –ñ–ö: {complex_gallery_images[:3]}")
                    except Exception as e:
                        print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ñ–ö: {e}")
                        import traceback
                        traceback.print_exc()
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É "–û –ñ–ö" –¥–ª—è —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞
                    try:
                        about_href = await page.evaluate("""
                        () => {
                          // –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–∫–∏ "–û –ñ–ö"
                          let a = document.querySelector('[data-e2e-id="complex-header-about"]');
                          console.log('Found by data-e2e-id:', !!a);
                          
                          if (!a) {
                            // –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ —Ç–µ–∫—Å—Ç—É
                            const links = Array.from(document.querySelectorAll('a'));
                            a = links.find(link => {
                              const text = (link.textContent || '').toLowerCase().trim();
                              return text.includes('–æ –∂–∫') || text.includes('–æ –∫–æ–º–ø–ª–µ–∫—Å–µ') || text.includes('–ø–æ–¥—Ä–æ–±–Ω–µ–µ');
                            });
                            console.log('Found by text:', !!a);
                          }
                          if (!a) {
                            // –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É, —Å–æ–¥–µ—Ä–∂–∞—â—É—é "about" –∏–ª–∏ "o-zhk"
                            const links = Array.from(document.querySelectorAll('a[href*="about"], a[href*="o-zhk"]'));
                            if (links.length > 0) {
                              a = links[0];
                            }
                            console.log('Found by href pattern:', !!a);
                          }
                          if (!a) {
                            // –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–º–ø–ª–µ–∫—Å–∞ —Å –ø—É—Ç–µ–º /about –∏–ª–∏ /o-zhk
                            const currentPath = location.pathname;
                            const basePath = currentPath.split('/').slice(0, -1).join('/'); // –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
                            const links = Array.from(document.querySelectorAll(`a[href*="${basePath}/about"], a[href*="${basePath}/o-zhk"]`));
                            if (links.length > 0) {
                              a = links[0];
                            }
                            console.log('Found by base path:', !!a);
                          }
                          if (a) {
                            const href = a.getAttribute('href') || a.href || null;
                            console.log('Found href:', href);
                            if (href) {
                              // –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π
                              try {
                                return new URL(href, location.origin).href;
                              } catch {
                                return href.startsWith('http') ? href : location.origin + (href.startsWith('/') ? href : '/' + href);
                              }
                            }
                          }
                          console.log('No link found, returning null');
                          return null;
                        }
                        """)
                        print(f"  üîç –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–∫–∏ '–û –ñ–ö': {about_href}")
                        if about_href:
                            print(f"  –û –ñ–ö URL: {about_href}")
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —É–∂–µ URL –ø—É—Ç—å –∫ —Ö–æ–¥—É —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞
                            if '/hod-stroitelstva' in about_href:
                                aggregated_hod_url = about_href
                                print(f"  –•–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ URL (—É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—É—Ç—å): {aggregated_hod_url}")
                            elif about_href.endswith('/'):
                                aggregated_hod_url = about_href + 'hod-stroitelstva'
                                print(f"  –•–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ URL: {aggregated_hod_url}")
                            else:
                                aggregated_hod_url = about_href + '/hod-stroitelstva'
                                print(f"  –•–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ URL: {aggregated_hod_url}")
                        else:
                            print(f"  ‚ö†Ô∏è –°—Å—ã–ª–∫–∞ '–û –ñ–ö' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ. –ü—Ä–æ–±—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±...")
                            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±: —Ñ–æ—Ä–º–∏—Ä—É–µ–º URL –Ω–∞–ø—Ä—è–º—É—é –∏–∑ URL –∫–æ–º–ø–ª–µ–∫—Å–∞
                            if aggregated_complex_href:
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —É–∂–µ URL –ø—É—Ç—å –∫ —Ö–æ–¥—É —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞
                                if '/hod-stroitelstva' in aggregated_complex_href:
                                    aggregated_hod_url = aggregated_complex_href
                                    print(f"  –•–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ URL (—É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—É—Ç—å): {aggregated_hod_url}")
                                elif aggregated_complex_href.endswith('/'):
                                    aggregated_hod_url = aggregated_complex_href + 'hod-stroitelstva'
                                    print(f"  –•–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ URL (—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏): {aggregated_hod_url}")
                                else:
                                    aggregated_hod_url = aggregated_complex_href + '/hod-stroitelstva'
                                    print(f"  –•–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ URL (—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏): {aggregated_hod_url}")
                    except Exception as e:
                        print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ö–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞: {e}")
                        # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                        if aggregated_complex_href:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —É–∂–µ URL –ø—É—Ç—å –∫ —Ö–æ–¥—É —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞
                            if '/hod-stroitelstva' in aggregated_complex_href:
                                aggregated_hod_url = aggregated_complex_href
                                print(f"  –•–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ URL (—É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—É—Ç—å): {aggregated_hod_url}")
                            elif aggregated_complex_href.endswith('/'):
                                aggregated_hod_url = aggregated_complex_href + 'hod-stroitelstva'
                                print(f"  –•–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ URL (—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏): {aggregated_hod_url}")
                            else:
                                aggregated_hod_url = aggregated_complex_href + '/hod-stroitelstva'
                                print(f"  –•–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ URL (—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏): {aggregated_hod_url}")
                except Exception as e:
                    print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–æ–º–ø–ª–µ–∫—Å–∞: {e}")

            # —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å –ø–æ–¥ Mongo-—Å—Ö–µ–º—É
            def to_db_item(complex_photos_urls: List[str] = None, processed_apartment_types: Dict[str, Any] = None) -> \
                    Dict[str, Any]:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–≤–∞—Ä—Ç–∏—Ä, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                apartment_types_data = processed_apartment_types or aggregated_offers

                apartment_types: Dict[str, Any] = {}
                for group, cards in (apartment_types_data or {}).items():
                    # cards –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–∞–∫ —Å–ø–∏—Å–∫–æ–º, —Ç–∞–∫ –∏ —Å–ª–æ–≤–∞—Ä–µ–º —Å –∫–ª—é—á–æ–º "apartments"
                    if isinstance(cards, list):
                        # –ï—Å–ª–∏ cards - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –∫–≤–∞—Ä—Ç–∏—Ä –Ω–∞–ø—Ä—è–º—É—é (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö)
                        apartment_types[group] = {
                            "apartments": [
                                {
                                    "title": c.get("offer"),
                                    "photos": c.get("photos") or [],  # URL –∫ —Ñ–∞–π–ª–∞–º –≤ S3
                                }
                                for c in cards
                            ]
                        }
                    elif isinstance(cards, dict) and "apartments" in cards:
                        # –ï—Å–ª–∏ cards - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º "apartments"
                        apartment_list = cards["apartments"]
                        apartment_types[group] = {
                            "apartments": [
                                {
                                    "title": c.get("offer"),
                                    "photos": c.get("photos") or [],  # URL –∫ —Ñ–∞–π–ª–∞–º –≤ S3
                                }
                                for c in apartment_list
                            ]
                        }
                    else:
                        # –ï—Å–ª–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        apartment_types[group] = cards
                        continue
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è
                complex_url = normalize_complex_url(aggregated_complex_href) if aggregated_complex_href else None
                if not complex_url:
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º base_url
                    complex_url = base_url
                
                return {
                    "url": complex_url,
                    "development": {
                        "complex_name": aggregated_complex_name,
                        "address": aggregated_address,
                        "source_url": base_url,
                        "photos": complex_photos_urls or [],  # URL –∫ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º –ñ–ö –≤ S3
                    },
                    "apartment_types": apartment_types,
                }

            # –ü–æ–ª—É—á–∞–µ–º ID –∫–æ–º–ø–ª–µ–∫—Å–∞ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª—é—á–µ–π S3
            complex_id = get_complex_id_from_url(aggregated_complex_href or base_url)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ñ–ö –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ S3
            complex_photos_urls = []
            if complex_gallery_images:
                try:
                    complex_photos_urls = await process_complex_photos(complex_gallery_images, complex_id)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ñ–ö: {e}")
                    complex_photos_urls = []

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –≤—Å–µ—Ö –∫–≤–∞—Ä—Ç–∏—Ä –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ S3
            processed_apartment_types = aggregated_offers
            if aggregated_offers:
                try:
                    processed_apartment_types = await process_all_apartment_types(aggregated_offers, complex_id)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∫–≤–∞—Ä—Ç–∏—Ä: {e}")
                    processed_apartment_types = aggregated_offers

            # –ü–æ—Å–ª–µ —Å–±–æ—Ä–∞ –≤—Å–µ—Ö –æ—Ñ—Ñ–µ—Ä–æ–≤: –µ—Å–ª–∏ –µ—Å—Ç—å hod_url ‚Äî –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∏ —Å–æ–±–∏—Ä–∞–µ–º —Ö–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞.
            # –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö (–ø—Ä–æ–∫—Å–∏/—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ) ‚Äî –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –∏ –ø—Ä–æ–±—É–µ–º –µ—â—ë —Ä–∞–∑.
            if aggregated_hod_url:
                print(f"  –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –¥–ª—è URL: {aggregated_hod_url}")
            else:
                print(f"  ‚ö†Ô∏è URL —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞—é —Å–±–æ—Ä")
            
            if aggregated_hod_url:
                complex_id = get_complex_id_from_url(aggregated_complex_href or base_url)
                max_attempts_hod = 3
                attempt_hod = 0
                while attempt_hod < max_attempts_hod and not construction_progress_data:
                    attempt_hod += 1
                    try:
                        print(f"  –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞: {aggregated_hod_url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt_hod}/{max_attempts_hod})")
                        stages_data = await extract_construction_from_domclick(page, aggregated_hod_url)
                        if stages_data and stages_data.get('construction_stages'):
                            print(f"  –ù–∞–π–¥–µ–Ω–æ —ç—Ç–∞–ø–æ–≤: {len(stages_data['construction_stages'])}")
                            construction_progress_data = await process_construction_stages_domclick(stages_data['construction_stages'], complex_id)
                            break
                        else:
                            print("  ‚ö†Ô∏è –≠—Ç–∞–ø—ã –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞")
                            # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –±—Ä–∞—É–∑–µ—Ä –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –ø–æ–ø—ã—Ç–∫—É
                            if attempt_hod < max_attempts_hod:
                                try:
                                    browser, page, _ = await restart_browser(browser, headless=False)
                                except Exception:
                                    pass
                    except Exception as e:
                        print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞: {e}")
                        if attempt_hod < max_attempts_hod:
                            try:
                                browser, page, _ = await restart_browser(browser, headless=False)
                                print("  üîÑ –ë—Ä–∞—É–∑–µ—Ä –ø–µ—Ä–µ–∑–∞–ø—É—â–µ–Ω –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏ —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞")
                            except Exception as restart_error:
                                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞: {restart_error}")

            db_item = to_db_item(complex_photos_urls, processed_apartment_types)
            if construction_progress_data:
                db_item.setdefault('development', {})['construction_progress'] = construction_progress_data

            try:
                save_to_mongodb([db_item])


            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ MongoDB: {e}. –°–æ—Ö—Ä–∞–Ω—é –≤ {str(OUTPUT_FILE)} –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.")
                results.append({"sourceUrl": base_url,
                                "data": {"address": aggregated_address, "complexName": aggregated_complex_name,
                                         "complexHref": aggregated_complex_href, "offers": processed_apartment_types,
                                         "complexPhotosUrls": complex_photos_urls}})
                with open(str(OUTPUT_FILE), 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)

            url_index += 1
            offset = 0
            save_progress(url_index, offset, str(PROGRESS_FILE))
    finally:
        try:
            await browser.close()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –∑–∞–∫—Ä—ã—Ç–∏—è –±—Ä–∞—É–∑–µ—Ä–∞


if __name__ == "__main__":
    asyncio.run(run())
