#!/usr/bin/env python3
"""
–ü—Ä–æ—Ö–æ–¥ –ø–æ —Å–ø–∏—Å–∫—É URL –∏ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö (—Ç–æ–ª—å–∫–æ 1-–π —ç–ª–µ–º–µ–Ω—Ç).

–õ–æ–≥–∏–∫–∞:
- –ß–∏—Ç–∞–µ—Ç JSON —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ (complex_links.json –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
- –î–ª—è –∫–∞–∂–¥–æ–≥–æ URL –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
- –ü–µ—Ä–µ—Ö–æ–¥–∏—Ç –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º offset (—à–∞–≥ 20): 0, 20, 40, ...
- –ù–∞ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∂–¥—ë—Ç [data-e2e-id="offers-list__item"], –±–µ—Ä—ë—Ç –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∏
  —Å–æ–±–∏—Ä–∞–µ—Ç: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –≥–∞–ª–µ—Ä–µ–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã, –∞–¥—Ä–µ—Å, –Ω–∞–∑–≤–∞–Ω–∏–µ/—Å—Å—ã–ª–∫—É –ñ–ö
- –ù–∞ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ø–æ–∏—Å–∫–∞ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ñ–ö –∏–∑ –≥–∞–ª–µ—Ä–µ–∏
  (—ç–ª–µ–º–µ–Ω—Ç—ã —Å data-e2e-id="complex-header-gallery-image__X")
- –°–∫–∞—á–∏–≤–∞–µ—Ç –í–°–ï —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ (–ñ–ö + –∫–≤–∞—Ä—Ç–∏—Ä), –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–µ—Ä–µ–∑ resize_img.py (—Å–∂–∞—Ç–∏–µ, –æ—á–∏—Å—Ç–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ –≤ –ø–∞–ø–∫–µ uploads/ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—É—Ç–∏ –≤ MongoDB:
  - development.photos - –ø—É—Ç–∏ –∫ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º –ñ–ö
  - apartment_types.*.apartments.*.photos - –ø—É—Ç–∏ –∫ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º –∫–≤–∞—Ä—Ç–∏—Ä
- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∏—à–µ—Ç –≤ MongoDB –∏ offers_data.json (–º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤)
- –ü—Ä–æ–≥—Ä–µ—Å—Å —Ö—Ä–∞–Ω–∏—Ç –≤ progress_domclick_2.json: {"url_index": i, "offset": n}
- –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö –¥–µ–ª–∞–µ—Ç –¥–æ 3 –ø–æ–ø—ã—Ç–æ–∫; –ø–æ—Å–ª–µ 3-–π ‚Äî –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ—Ç –±—Ä–∞—É–∑–µ—Ä (–Ω–æ–≤—ã–π –ø—Ä–æ–∫—Å–∏)
  –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Å —Ç–æ–≥–æ –∂–µ –º–µ—Å—Ç–∞
"""
import asyncio
import json
import os
import base64
import logging
from typing import List, Dict, Any, Tuple
from pathlib import Path
import aiohttp
from io import BytesIO
import sys
import shutil
from urllib.parse import urlparse

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ç–µ–∫—É—â–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
PROJECT_ROOT = Path(__file__).resolve().parent

# –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
UPLOADS_DIR = PROJECT_ROOT / "uploads"

from browser_manager import create_browser, create_browser_page, restart_browser
from db_manager import save_to_mongodb
from resize_img import ImageProcessor

LINKS_FILE = PROJECT_ROOT / "complex_links.json"
PROGRESS_FILE = PROJECT_ROOT / "progress_domclick_2.json"
OUTPUT_FILE = PROJECT_ROOT / "offers_data.json"  # –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π, –æ—Å—Ç–∞–≤–∏–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
START_PAUSE_SECONDS = 5  # –ø–∞—É–∑–∞ –ø–æ—Å–ª–µ –æ—Ç–∫—Ä—ã—Ç–∏—è URL
STEP_PAUSE_SECONDS = 5  # –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏/—à–∞–≥–∞–º–∏

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è ImageProcessor
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
image_processor = ImageProcessor(logger, max_size=(800, 600), max_kb=150)


def create_complex_directory(complex_id: str) -> Path:
    """
    –°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–∞.
    """
    complex_dir = UPLOADS_DIR / "complexes" / complex_id
    complex_photos_dir = complex_dir / "complex_photos"
    apartments_dir = complex_dir / "apartments"

    # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–ø–∫–∏
    complex_photos_dir.mkdir(parents=True, exist_ok=True)
    apartments_dir.mkdir(parents=True, exist_ok=True)

    return complex_dir


def get_complex_id_from_url(url: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç ID –∫–æ–º–ø–ª–µ–∫—Å–∞ –∏–∑ URL.
    """
    try:
        # –ü—Ä–∏–º–µ—Ä: https://domclick.ru/complexes/zhk-8-marta__109690
        parsed = urlparse(url)
        path_parts = parsed.path.split('/')
        if 'complexes' in path_parts:
            complex_index = path_parts.index('complexes')
            if complex_index + 1 < len(path_parts):
                return path_parts[complex_index + 1]
    except Exception:
        pass

    # Fallback - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ö–µ—à URL
    import hashlib
    return hashlib.md5(url.encode()).hexdigest()[:10]


async def extract_construction_from_domclick(page, hod_url: str) -> Dict[str, Any]:
    """–ü–µ—Ä–µ—Ö–æ–¥–∏—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ Domclick –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—ã –∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ—Ç–æ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç { construction_stages: [{stage_number, date, photos: [urls<=5]}] }.
    """
    script = """
    async (targetUrl) => {
      try {
        // –ù–∞–≤–∏–≥–∞—Ü–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É "–•–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞"
        if (location.href !== targetUrl) {
          history.scrollRestoration = 'manual';
        }
      } catch (e) {}
      return null;
    }
    """
    try:
        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞
        await page.goto(hod_url, timeout=120000)
        await asyncio.sleep(3)

        # –ö–ª–∏–∫ –ø–æ –±–µ–π–¥–∂—É –∏ –ø–æ —á–µ–∫–±–æ–∫—Å—É "2025" –≤ –û–î–ù–û–ú evaluate (—Å –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏)
        try:
            clicked_2025 = await page.evaluate(r"""
            async () => {
              const sleep = (ms) => new Promise(r => setTimeout(r, ms));
              // 1) –ö–ª–∏–∫ –ø–æ –±–µ–π–¥–∂—É
              const badge = document.querySelector('[data-badge="true"]');
              if (badge) { badge.click(); await sleep(300); }

              // 2) –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ü–∏—é 2025
              const normalize = (s) => String(s || '').replace(/\s+/g, ' ').trim();
              const options = Array.from(document.querySelectorAll('[role="option"], [aria-selected]'));
              const opt2025 = options.find(el => /\b2025\b/.test(normalize(el.textContent)));
              if (!opt2025) return false;

              // 3) –ò—â–µ–º –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
              const checkbox = opt2025.querySelector('input[type="checkbox"]');
              const target = checkbox || opt2025.querySelector('label, [role="checkbox"], .checkbox-root, .list-cell-root, span[tabindex], div[tabindex]') || opt2025;

              // 4) –≠–º—É–ª—è—Ü–∏—è –∫–ª–∏–∫–∞
              const fire = (type, el) => el && el.dispatchEvent(new MouseEvent(type, { bubbles: true, cancelable: true, view: window }));
              await sleep(150); fire('pointerover', target);
              await sleep(150); fire('mouseover',  target);
              await sleep(180); fire('pointerdown', target);
              await sleep(150); fire('mousedown',   target);
              await sleep(220); fire('pointerup',   target);
              await sleep(180); fire('mouseup',     target);
              await sleep(220);
              return target.dispatchEvent(new MouseEvent('click', { bubbles: true, cancelable: true, view: window }));
            }
            """)
            if clicked_2025:
                await asyncio.sleep(1200/1000)
        except Exception:
            pass

        try:
            await page.waitForSelector('[data-testid="construction-progress-pagination"]', {"timeout": 4000})
        except Exception:
            pass
    except Exception:
        return {"construction_stages": []}

    eval_script = r"""
    () => {
      const toAbs = (u) => { try { return new URL(u, location.origin).href; } catch { return u || null; } };
      const isImg = (u) => /\.(png|jpe?g|webp)(?:$|\?|#)/i.test(String(u || ''));
      const pickFromSrcset = (srcset) => {
        if (!srcset) return null;
        const first = String(srcset).split(',')[0].trim().split(' ')[0];
        return first || null;
      };
      const headerLike = (txt) => {
        if (!txt) return false;
        const s = txt.replace(/\s+/g, ' ').trim();
        if (s.length < 5 || s.length > 160) return false;
        const hasMarkers = /(–∫–≤–∞—Ä—Ç–∞–ª|–∫–≤\.|–ª–∏—Ç–µ—Ä|–æ–±–Ω–æ–≤–ª–µ–Ω|–æ–±–Ω–æ–≤–ª–µ–Ω–æ|–≥–æ–¥|–º–µ—Å—è—Ü)/i.test(s);
        const hasYear = /\b20\d{2}\b/.test(s);
        const hasMonthYear = /[–ê-–Ø–Å][–∞-—è—ë]+,?\s*\d{4}/.test(s);
        return hasMarkers || hasYear || hasMonthYear;
      };
      const collectImages = (root) => {
        const urls = new Set();
        root.querySelectorAll('img').forEach(img => {
          const s1 = img.getAttribute('src');
          const s2 = img.getAttribute('data-src') || img.getAttribute('data-lazy') || img.getAttribute('data-original');
          const s3 = pickFromSrcset(img.getAttribute('srcset'));
          [s1, s2, s3].filter(Boolean).map(toAbs).filter(isImg).forEach(u => urls.add(u));
        });
        root.querySelectorAll('source[srcset]').forEach(s => {
          const picked = pickFromSrcset(s.getAttribute('srcset'));
          if (picked && isImg(picked)) urls.add(toAbs(picked));
        });
        root.querySelectorAll('[style*=\"background\"]').forEach(el => {
          const st = String(el.getAttribute('style') || '');
          const m = st.match(/url\((['\"]?)(.*?)\1\)/i);
          if (m && isImg(m[2])) urls.add(toAbs(m[2]));
        });
        return [...urls];
      };

      const pagination = document.querySelector('[data-testid=\"construction-progress-pagination\"]');
      const container = pagination ? pagination.parentElement : null;
      let upperBlocks = [];
      if (container && pagination) {
        let el = pagination.previousElementSibling;
        while (el) { upperBlocks.push(el); el = el.previousElementSibling; }
        upperBlocks.reverse();
      }
      if (!upperBlocks.length) {
        const candidate = document.querySelector('[role=\"list\"] [role=\"listitem\"]')
          ? document.querySelector('[role=\"list\"]').parentElement
          : document.body;
        upperBlocks = [candidate];
      }

      const seen = new Set();
      const stages = [];

      const extractStageFromBlock = (block) => {
        const headerEl = Array.from(block.querySelectorAll('div,span,p,h1,h2,h3,h4'))
          .find(x => headerLike((x.innerText || '').replace(/\s+/g, ' ').trim()));
        const title = headerEl ? (headerEl.innerText || '').replace(/\s+/g, ' ').trim() : null;
        const photos = collectImages(block).slice(0, 5);
        if (!(title || photos.length)) return;
        const key = `${title || ''}::${photos[0] || ''}`;
        if (!seen.has(key)) {
          stages.push({ title: title || '–≠—Ç–∞–ø', photos });
          seen.add(key);
        }
      };

      upperBlocks.forEach(extractStageFromBlock);
      const filtered = stages.filter(s => s.photos && s.photos.length);
      return filtered;
    }
    """
    # –°–±–æ—Ä —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    stages_merged: List[Dict[str, Any]] = []
    used_keys = set()

    def merge_pages(stages_page: List[Dict[str, Any]]):
        for s in stages_page or []:
            title = s.get('title') or s.get('date') or ''
            photos = list(s.get('photos') or [])[:5]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–º–∏ 5
            key = f"{title}::{photos[0] if photos else ''}"
            if key in used_keys:
                continue
            used_keys.add(key)
            stages_merged.append({
                'stage_number': len(stages_merged) + 1,
                'date': title,
                'photos': photos
            })

    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
        pages_count = await page.evaluate("""
        () => {
          const pag = document.querySelector('[data-testid="construction-progress-pagination"]');
          if (!pag) return 1;
          const nums = Array.from(pag.querySelectorAll('button, a'))
            .map(el => parseInt((el.textContent || '').trim(), 10))
            .filter(n => Number.isFinite(n));
          return Math.max(1, ...(nums.length ? nums : [1]));
        }
        """)
        if not isinstance(pages_count, (int, float)) or pages_count < 1:
            pages_count = 1

        for page_index in range(1, int(pages_count) + 1):
            try:
                data = await page.evaluate(eval_script)
                if isinstance(data, list):
                    merge_pages(data)
                elif isinstance(data, dict):
                    merge_pages(data.get('stages') or data.get('construction_stages') or [])
            except Exception:
                pass

            # –ö–ª–∏–∫–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É, –µ—Å–ª–∏ –µ—Å—Ç—å
            if page_index < pages_count:
                try:
                    clicked = await page.evaluate("""
                    (n) => {
                      const pag = document.querySelector('[data-testid="construction-progress-pagination"]');
                      if (!pag) return false;
                      const btn = Array.from(pag.querySelectorAll('button, a'))
                        .find(el => (el.textContent || '').trim() === String(n));
                      if (btn) { btn.click(); return true; }
                      return false;
                    }
                    """, page_index + 1)
                    if clicked:
                        await asyncio.sleep(2)
                except Exception:
                    pass

        return {"construction_stages": stages_merged}
    except Exception:
        return {"construction_stages": []}


async def process_construction_stages_domclick(stages: List[Dict[str, Any]], complex_dir: Path) -> Dict[str, Any]:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–æ—Ç–æ –ø–æ —ç—Ç–∞–ø–∞–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É construction_progress —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –ø—É—Ç—è–º–∏."""
    if not stages:
        return {"construction_stages": []}
    base_constr = complex_dir / "construction"
    result_stages = []
    async with aiohttp.ClientSession() as session:
        for s in stages:
            stage_num = s.get("stage_number") or (len(result_stages) + 1)
            stage_dir = base_constr / f"stage_{stage_num}"
            urls = (s.get("photos") or [])[:5]  # —Å–∫–∞—á–∏–≤–∞–µ–º –Ω–µ –±–æ–ª–µ–µ 5 —Ñ–æ—Ç–æ –Ω–∞ —ç—Ç–∞–ø
            saved = []
            sem = asyncio.Semaphore(5)
            async def work(u, idx):
                async with sem:
                    fp = stage_dir / f"photo_{idx+1}.jpg"
                    return await download_and_process_image(session, u, fp)
            tasks = [work(u, i) for i, u in enumerate(urls)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for p in results:
                if isinstance(p, str) and p:
                    saved.append(p)
            result_stages.append({
                "stage_number": stage_num,
                "date": s.get("date") or "",
                "photos": saved
            })
    return {"construction_stages": result_stages}


def save_processed_image(image_data: bytes, file_path: Path) -> bool:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª.
    """
    try:
        file_path.parent.mkdir(parents=True, exist_ok=True)
        with open(file_path, 'wb') as f:
            f.write(image_data)
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return False


def load_links(path: str = str(LINKS_FILE)) -> List[str]:
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    # –¥–æ–ø—É—Å–∫–∞–µ–º –∫–∞–∫ —Å–ø–∏—Å–æ–∫, —Ç–∞–∫ –∏ —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º links
    if isinstance(data, dict) and "links" in data:
        return list(data.get("links") or [])
    return list(data or [])


def load_progress(path: str = str(PROGRESS_FILE)) -> Tuple[int, int]:
    if not os.path.exists(path):
        return 0, 0
    try:
        with open(path, "r", encoding="utf-8") as f:
            obj = json.load(f)
        return int(obj.get("url_index", 0)), int(obj.get("offset", 0))
    except Exception:
        return 0, 0


def save_progress(url_index: int, offset: int, path: str = str(PROGRESS_FILE)) -> None:
    tmp_path = path + ".tmp"
    with open(tmp_path, "w", encoding="utf-8") as f:
        json.dump({"url_index": url_index, "offset": offset}, f, ensure_ascii=False)
    os.replace(tmp_path, path)


def set_offset_param(url: str, offset: int) -> str:
    try:
        from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse
        parts = urlparse(url)
        q = dict(parse_qsl(parts.query, keep_blank_values=True))
        q["offset"] = str(offset)
        new_query = urlencode(q, doseq=True)
        return urlunparse((parts.scheme, parts.netloc, parts.path, parts.params, new_query, parts.fragment))
    except Exception:
        sep = '&' if ('?' in url) else '?'
        return f"{url}{sep}offset={offset}"


async def download_and_process_image(session: aiohttp.ClientSession, image_url: str, file_path: Path) -> str:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ URL, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –µ–≥–æ —á–µ—Ä–µ–∑ resize_img.py –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É.
    """
    try:
        async with session.get(image_url, timeout=aiohttp.ClientTimeout(total=30)) as response:
            if response.status == 200:
                image_bytes = await response.read()

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ resize_img.py
                input_bytes = BytesIO(image_bytes)
                try:
                    processed_bytes = image_processor.process(input_bytes)
                except Exception as process_error:
                    logger.error(f"–û—à–∏–±–∫–∞ resize_img.py: {process_error}")
                    return None

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
                processed_bytes.seek(0)
                image_data = processed_bytes.read()

                if save_processed_image(image_data, file_path):
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç uploads
                    relative_path = file_path.relative_to(UPLOADS_DIR)
                    return str(relative_path).replace('\\', '/')  # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
                else:
                    return None
            else:
                logger.warning(f"HTTP {response.status} –¥–ª—è {image_url}")
                return None
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {image_url}: {e}")
        return None


async def process_complex_photos(photo_urls: List[str], complex_dir: Path) -> List[str]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ñ–ö –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –ª–æ–∫–∞–ª—å–Ω–æ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º.
    """
    if not photo_urls:
        return []

    processed_photos = []
    complex_photos_dir = complex_dir / "complex_photos"

    async with aiohttp.ClientSession() as session:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ 5 —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        semaphore = asyncio.Semaphore(5)

        async def process_single_photo(url, index):
            async with semaphore:
                file_path = complex_photos_dir / f"photo_{index + 1}.jpg"
                return await download_and_process_image(session, url, file_path)

        tasks = [process_single_photo(url, i) for i, url in enumerate(photo_urls[:8])]  # –º–∞–∫—Å–∏–º—É–º 8 —Ñ–æ—Ç–æ –ñ–ö
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, str) and result:
                processed_photos.append(result)

    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(processed_photos)} –∏–∑ {len(photo_urls)} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ñ–ö")
    return processed_photos


async def process_apartment_photos(apartment_data: Dict[str, Any], apartment_dir: Path) -> Dict[str, Any]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –¥–ª—è –æ–¥–Ω–æ–π –∫–≤–∞—Ä—Ç–∏—Ä—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –ª–æ–∫–∞–ª—å–Ω–æ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å –ø—É—Ç—è–º–∏ –∫ —Ñ–∞–π–ª–∞–º.
    """
    if not apartment_data.get("images"):
        return {
            "offer": apartment_data.get("offer"),
            "photos": []
        }

    image_urls = apartment_data["images"]
    if not image_urls:
        return {
            "offer": apartment_data.get("offer"),
            "photos": []
        }

    processed_images = []

    async with aiohttp.ClientSession() as session:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ 3 —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –∫–≤–∞—Ä—Ç–∏—Ä
        semaphore = asyncio.Semaphore(3)

        async def process_single_photo(url, index):
            async with semaphore:
                file_path = apartment_dir / f"photo_{index + 1}.jpg"
                return await download_and_process_image(session, url, file_path)

        tasks = [process_single_photo(url, i) for i, url in enumerate(image_urls[:3])]  # –º–∞–∫—Å–∏–º—É–º 3 —Ñ–æ—Ç–æ –Ω–∞ –∫–≤–∞—Ä—Ç–∏—Ä—É
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for i, result in enumerate(results):
            if isinstance(result, str) and result:
                processed_images.append(result)
            elif isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ {i + 1}: {result}")
            else:
                logger.warning(f"–§–æ—Ç–æ {i + 1} –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {type(result)}")

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–≤–∞—Ä—Ç–∏—Ä—ã —Å –ø—É—Ç—è–º–∏ –∫ —Ñ–∞–π–ª–∞–º
    result = {
        "offer": apartment_data.get("offer"),
        "photos": processed_images
    }
    return result


async def process_all_apartment_types(apartment_types: Dict[str, Any], complex_dir: Path) -> Dict[str, Any]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –≤–æ –≤—Å–µ—Ö —Ç–∏–ø–∞—Ö –∫–≤–∞—Ä—Ç–∏—Ä –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –ª–æ–∫–∞–ª—å–Ω–æ.
    """
    if not apartment_types:
        return apartment_types

    processed_types = {}
    apartments_dir = complex_dir / "apartments"

    for apartment_type, type_data in apartment_types.items():
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        if isinstance(type_data, list):
            # –ï—Å–ª–∏ type_data - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –∫–≤–∞—Ä—Ç–∏—Ä –Ω–∞–ø—Ä—è–º—É—é
            apartments = type_data
        elif isinstance(type_data, dict) and "apartments" in type_data:
            # –ï—Å–ª–∏ type_data - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º "apartments"
            apartments = type_data.get("apartments", [])
        else:
            # –ï—Å–ª–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            processed_types[apartment_type] = type_data
            continue

        processed_apartments = []
        apartment_type_dir = apartments_dir / apartment_type.replace('-', '_').replace('–∫–æ–º–Ω', 'komn')

        for i, apartment in enumerate(apartments):
            if isinstance(apartment, dict):
                apartment_dir = apartment_type_dir / f"apartment_{i + 1}"
                processed_apartment = await process_apartment_photos(apartment, apartment_dir)
                processed_apartments.append(processed_apartment)
            else:
                processed_apartments.append(apartment)

        # –ü—Ä–∞–≤–∏–ª—å–Ω–æ —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        if isinstance(type_data, list):
            # –ï—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —Å–ø–∏—Å–∫–æ–º, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫
            processed_types[apartment_type] = processed_apartments
        else:
            # –ï—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —Å–ª–æ–≤–∞—Ä–µ–º, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
            processed_types[apartment_type] = {
                **type_data,
                "apartments": processed_apartments
            }

    return processed_types


class SkipUrlException(Exception):
    pass


async def wait_offers(page) -> None:
    try:
        await page.waitForSelector('[data-e2e-id="offers-list__item"]', {"timeout": 60000})
    except Exception as e:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –æ—à–∏–±–∫–æ–π "–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫..."
        try:
            error_page = await page.evaluate("""
            () => {
              const t = (document.body && document.body.innerText) || '';
              return /–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫/i.test(t) || /–í —Ä–∞–±–æ—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –ø—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞/i.test(t);
            }
            """)
        except Exception:
            error_page = False

        if error_page:
            raise Exception("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å –æ—à–∏–±–∫–æ–π Domclick - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–∫—Ä–∞–Ω "–ü–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
        try:
            no_results = await page.evaluate("""
            () => {
              const t = (document.body && document.body.innerText) || '';
              return /–ü–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤/i.test(t);
            }
            """)
        except Exception:
            no_results = False
        if no_results:
            raise SkipUrlException("–ü–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        # –ò–Ω–∞—á–µ —Ç—Ä–∏–≥–≥–µ—Ä–∏–º —Ä–µ—Ç—Ä–∞–∏/–ø–µ—Ä–µ–∑–∞–ø—É—Å–∫
        raise TimeoutError("–ù–µ –Ω–∞–π–¥–µ–Ω —Å–ø–∏—Å–æ–∫ –æ—Ñ—Ñ–µ—Ä–æ–≤ [data-e2e-id='offers-list__item']") from e


async def get_pages_count(page) -> int:
    script = r"""
    () => {
      const nodes = Array.from(document.querySelectorAll('[data-e2e-id^="paginate-item-"]'));
      return nodes.length || 1;
    }
    """
    try:
        count = await page.evaluate(script)
        return int(count) if isinstance(count, (int, float)) else 1
    except Exception:
        return 1


async def collect_page_items_grouped(page) -> Dict[str, Any]:
    script = """
    () => {
      const toAbs = (u) => { try { return new URL(u, location.origin).href; } catch { return u || null; } };
      const isPngJpg = (u) => /\.(png|jpe?g)(?:$|\?|#)/i.test(String(u || ''));
      const pickFromSrcset = (srcset) => {
        if (!srcset) return null;
        const first = String(srcset).split(',')[0].trim().split(' ')[0];
        return first || null;
      };
      const extractImagesFromGallery = (root) => {
        if (!root) return [];
        const urls = new Set();
        root.querySelectorAll('img').forEach(img => {
          [img.getAttribute('src'), img.getAttribute('data-src'), img.getAttribute('data-lazy'), img.getAttribute('data-original')]
            .filter(Boolean).map(toAbs).filter(isPngJpg).forEach(u => urls.add(u));
          const picked = pickFromSrcset(img.getAttribute('srcset'));
          if (isPngJpg(picked)) urls.add(toAbs(picked));
        });
        root.querySelectorAll('source[srcset]').forEach(s => {
          const picked = pickFromSrcset(s.getAttribute('srcset'));
          if (isPngJpg(picked)) urls.add(toAbs(picked));
        });
        root.querySelectorAll("[style*='background']").forEach(el => {
          const st = el.getAttribute('style') || '';
          const m = st.match(/url\((['\"]?)(.*?)\1\)/i);
          if (m && isPngJpg(m[2])) urls.add(toAbs(m[2]));
        });
        return [...urls];
      };

      const normalizeRoom = (txt) => {
        if (!txt) return '–î—Ä—É–≥–æ–µ';
        const s = txt.toLowerCase();
        if (s.includes('—Å—Ç—É–¥')) return '–°—Ç—É–¥–∏—è';
        const m = s.match(/(^|\s)([1-9]+)\s*[-‚Äì‚Äî]?\s*–∫–æ–º–Ω/i);
        if (m) return `${m[2]}-–∫–æ–º–Ω`;
        const m2 = s.match(/^([1-9]+)\s*[-‚Äì‚Äî]?\s*–∫–æ–º–º?/);
        if (m2) return `${m2[1]}-–∫–æ–º–Ω`;
        return '–î—Ä—É–≥–æ–µ';
      };

      const items = Array.from(document.querySelectorAll('[data-e2e-id="offers-list__item"]'));
      if (!items.length) return null;

      const first = items[0];
      const addressEl = first.querySelector('[data-e2e-id="product-snippet-address"], [data-e2-id="product-snippet-address"]');
      const addressText = addressEl ? addressEl.textContent.replace(/\s+/g, ' ').trim() : null;
      const complexLinkEl = first.querySelector('a[href^="/complexes/"], a[href*="//domclick.ru/complexes/"], a[href*="//ufa.domclick.ru/complexes/"]');
      const complexName = complexLinkEl ? complexLinkEl.textContent.replace(/\s+/g, ' ').trim() : null;
      const complexHref = complexLinkEl ? toAbs(complexLinkEl.getAttribute('href') || complexLinkEl.href) : null;

      const offers = {};
      for (const item of items) {
        const offerEl = item.querySelector('[data-test="product-snippet-property-offer"]');
        const offerText = offerEl ? offerEl.innerText.replace(/\s+/g, ' ').trim() : null;
        const roomKey = normalizeRoom(offerText);
        const galleryRoot = item.querySelector('[data-e2e-id="product-snippet-gallery"]') || item.querySelector('[data-e2e-id^="product-snippet-gallery"]') || item;
        const images = extractImagesFromGallery(galleryRoot);
        const card = { offer: offerText, images };
        if (!offers[roomKey]) offers[roomKey] = [];
        offers[roomKey].push(card);
      }

      // –∏–∑–≤–ª–µ–∫–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ñ–ö –∏–∑ –≥–∞–ª–µ—Ä–µ–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ø–æ–∏—Å–∫–∞
      const complexPhotos = [];
      const galleryContainer = document.querySelector('[data-e2e-id="complex-header-gallery"]');
      
      if (galleryContainer) {
        const imageElements = galleryContainer.querySelectorAll('[data-e2e-id^="complex-header-gallery-image__"]');
        imageElements.forEach(element => {
          const img = element.querySelector('img.picture-image-object-fit--cover-820-5-0-5.picture-imageFillingContainer-4a2-5-0-5');
          if (img && img.src) {
            try {
              const absoluteUrl = new URL(img.src, location.origin).href;
              complexPhotos.push(absoluteUrl);
            } catch (e) {
              if (img.src.startsWith('http')) {
                complexPhotos.push(img.src);
              }
            }
          }
        });
      }

      return { address: addressText, complexName, complexHref, offers, complexPhotos };
    }
    """
    try:
        return await page.evaluate(script)
    except Exception:
        return None


async def run() -> None:
    urls = load_links(str(LINKS_FILE))
    if not urls:
        print("–§–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –ø—É—Å—Ç –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç:", LINKS_FILE)
        return

    url_index, offset = load_progress(str(PROGRESS_FILE))
    url_index = max(0, min(url_index, len(urls)))
    print(f"–°—Ç–∞—Ä—Ç: url_index={url_index}, offset={offset}, –≤—Å–µ–≥–æ URL: {len(urls)}")

    results: List[Dict[str, Any]] = []
    if os.path.exists(str(OUTPUT_FILE)):
        try:
            with open(str(OUTPUT_FILE), 'r', encoding='utf-8') as f:
                old = json.load(f)
                if isinstance(old, list):
                    results = old
        except Exception:
            pass

    browser, _ = await create_browser(headless=False)
    page = await create_browser_page(browser)

    try:
        while url_index < len(urls):
            base_url = urls[url_index]
            print(f"‚Üí URL [{url_index + 1}/{len(urls)}]: {base_url}")

            if offset % 20 != 0:
                offset = (offset // 20) * 20

            attempts = 0
            while True:
                try:
                    await page.goto(set_offset_param(base_url, 0), timeout=120000)
                    # –ü–∞—É–∑–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏–∫–∏
                    await asyncio.sleep(START_PAUSE_SECONDS)
                    await wait_offers(page)
                    pages_count = await get_pages_count(page)
                    break
                except SkipUrlException as e:
                    print(f"URL –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}. –ü—Ä–æ–ø—É—Å–∫–∞—é {base_url}")
                    pages_count = 0
                    break
                except Exception as e:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –æ—à–∏–±–∫–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã Domclick
                    if "—Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å –æ—à–∏–±–∫–æ–π Domclick" in str(e):
                        print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å –æ—à–∏–±–∫–æ–π Domclick: {e}")
                        try:
                            browser, page, _ = await restart_browser(browser, headless=False)
                            attempts = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
                            continue  # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É —Å –Ω–æ–≤—ã–º –±—Ä–∞—É–∑–µ—Ä–æ–º
                        except Exception as restart_error:
                            print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ –±—Ä–∞—É–∑–µ—Ä–∞: {restart_error}")
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç URL –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É
                            break

                    attempts += 1
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ –±–∞–∑–æ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e} (–ø–æ–ø—ã—Ç–∫–∞ {attempts}/3)")
                    if attempts >= 3:
                        try:
                            browser, page, _ = await restart_browser(browser, headless=False)
                            attempts = 0
                        except Exception as restart_error:
                            print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ –±—Ä–∞—É–∑–µ—Ä–∞: {restart_error}")
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç URL –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É
                            break
                    else:
                        await asyncio.sleep(2)

            total_pages = max(1, pages_count)

            # –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º –æ—Ñ—Ñ—Å–µ—Ç–∞–º –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ URL
            aggregated_address: str = None
            aggregated_complex_name: str = None
            aggregated_complex_href: str = None
            aggregated_offers: Dict[str, List[Dict[str, Any]]] = {}
            complex_gallery_images: List[str] = []  # —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ñ–ö (–∏–∑–≤–ª–µ–∫–∞–µ–º –æ–¥–∏–Ω —Ä–∞–∑)
            aggregated_hod_url: str = None  # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞
            construction_progress_data: Dict[str, Any] = None

            while True:
                if offset >= total_pages * 20:
                    break
                current_url = set_offset_param(base_url, offset)
                print(f"  –ü–µ—Ä–µ—Ö–æ–¥: offset={offset} ‚Üí {current_url}")

                attempts = 0
                while True:
                    try:
                        await page.goto(current_url, timeout=120000)
                        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —à–∞–≥–∞–º–∏ –¥–ª—è –¥–æ–∫—Ä—É—Ç–∫–∏ –ª–µ–Ω–∏–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                        await asyncio.sleep(STEP_PAUSE_SECONDS)
                        await wait_offers(page)
                        data = await collect_page_items_grouped(page)
                        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã ‚Äî —Å—á–∏—Ç–∞–µ–º –∑–∞ –æ—à–∏–±–∫—É —à–∞–≥–∞
                        if not data or not (data.get("offers") or {}):
                            raise ValueError("–°–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–ª–∏ offers –ø—É—Å—Ç–æ–π")

                        if aggregated_address is None:
                            aggregated_address = data.get("address")
                            aggregated_complex_name = data.get("complexName")
                            aggregated_complex_href = data.get("complexHref")

                            # –∏–∑–≤–ª–µ–∫–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ñ–ö —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º —Å–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
                            if not complex_gallery_images:
                                complex_gallery_images = data.get("complexPhotos") or []
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É "–û –ñ–ö", —á—Ç–æ–±—ã –ø–æ–∑–∂–µ –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ /hod-stroitelstva
                            try:
                                about_href = await page.evaluate("""
                                () => {
                                  const a = document.querySelector('[data-e2e-id="complex-header-about"]');
                                  if (a) return a.getAttribute('href') || a.href || null;
                                  return null;
                                }
                                """)
                                if about_href:
                                    print(f"  –û –ñ–ö URL: {about_href}")
                                    if about_href.endswith('/'):
                                        aggregated_hod_url = about_href + 'hod-stroitelstva'
                                    else:
                                        aggregated_hod_url = about_href + '/hod-stroitelstva'
                                    print(f"  –•–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ URL: {aggregated_hod_url}")
                            except Exception as e:
                                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å—Å—ã–ª–∫—É –Ω–∞ —Ö–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞: {e}")

                        # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≥—Ä—É–ø–ø—ã –æ—Ñ—Ñ–µ—Ä–æ–≤
                        offers = data.get("offers") or {}
                        for group, cards in offers.items():
                            if group not in aggregated_offers:
                                aggregated_offers[group] = []
                            aggregated_offers[group].extend(cards)

                        offset += 20
                        # –ù–µ–±–æ–ª—å—à–∞—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –æ—Ñ—Ñ—Å–µ—Ç–æ–º
                        await asyncio.sleep(1)
                        save_progress(url_index, offset, str(PROGRESS_FILE))
                        break
                    except SkipUrlException as e:
                        print(f"Offset –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}. –ó–∞–≤–µ—Ä—à–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É URL: {base_url}")
                        offset = total_pages * 20
                        break
                    except Exception as e:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –æ—à–∏–±–∫–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã Domclick
                        if "—Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å –æ—à–∏–±–∫–æ–π Domclick" in str(e):
                            print(f"  –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å –æ—à–∏–±–∫–æ–π Domclick –Ω–∞ offset={offset}: {e}")
                            try:
                                browser, page, _ = await restart_browser(browser, headless=False)
                                attempts = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
                                # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É —Å –Ω–æ–≤—ã–º –±—Ä–∞—É–∑–µ—Ä–æ–º
                                continue
                            except Exception as restart_error:
                                print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ –±—Ä–∞—É–∑–µ—Ä–∞: {restart_error}")
                                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç offset –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É
                                offset += 20
                                break

                        attempts += 1
                        print(f"  –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ offset={offset}: {e} (–ø–æ–ø—ã—Ç–∫–∞ {attempts}/3)")
                        if attempts >= 3:
                            try:
                                browser, page, _ = await restart_browser(browser, headless=False)
                                attempts = 0
                            except Exception as restart_error:
                                print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ –±—Ä–∞—É–∑–µ—Ä–∞: {restart_error}")
                                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç offset –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É
                                offset += 20
                                break
                        else:
                            await asyncio.sleep(2)

            # —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å –ø–æ–¥ Mongo-—Å—Ö–µ–º—É
            def to_db_item(complex_photos_paths: List[str] = None, processed_apartment_types: Dict[str, Any] = None) -> \
                    Dict[str, Any]:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–≤–∞—Ä—Ç–∏—Ä, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                apartment_types_data = processed_apartment_types or aggregated_offers

                apartment_types: Dict[str, Any] = {}
                for group, cards in (apartment_types_data or {}).items():
                    # cards –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–∞–∫ —Å–ø–∏—Å–∫–æ–º, —Ç–∞–∫ –∏ —Å–ª–æ–≤–∞—Ä–µ–º —Å –∫–ª—é—á–æ–º "apartments"
                    if isinstance(cards, list):
                        # –ï—Å–ª–∏ cards - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –∫–≤–∞—Ä—Ç–∏—Ä –Ω–∞–ø—Ä—è–º—É—é (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö)
                        apartment_types[group] = {
                            "apartments": [
                                {
                                    "title": c.get("offer"),
                                    "photos": c.get("photos") or [],  # –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
                                }
                                for c in cards
                            ]
                        }
                    elif isinstance(cards, dict) and "apartments" in cards:
                        # –ï—Å–ª–∏ cards - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º "apartments"
                        apartment_list = cards["apartments"]
                        apartment_types[group] = {
                            "apartments": [
                                {
                                    "title": c.get("offer"),
                                    "photos": c.get("photos") or [],  # –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
                                }
                                for c in apartment_list
                            ]
                        }
                    else:
                        # –ï—Å–ª–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        apartment_types[group] = cards
                        continue
                return {
                    "url": aggregated_complex_href or base_url,
                    "development": {
                        "complex_name": aggregated_complex_name,
                        "address": aggregated_address,
                        "source_url": base_url,
                        "photos": complex_photos_paths or [],  # –ø—É—Ç–∏ –∫ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º –ñ–ö
                    },
                    "apartment_types": apartment_types,
                }

            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–∞
            complex_id = get_complex_id_from_url(aggregated_complex_href or base_url)
            complex_dir = create_complex_directory(complex_id)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ñ–ö –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ
            complex_photos_paths = []
            if complex_gallery_images:
                try:
                    complex_photos_paths = await process_complex_photos(complex_gallery_images, complex_dir)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ñ–ö: {e}")
                    complex_photos_paths = []

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –≤—Å–µ—Ö –∫–≤–∞—Ä—Ç–∏—Ä –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ
            processed_apartment_types = aggregated_offers
            if aggregated_offers:
                try:
                    processed_apartment_types = await process_all_apartment_types(aggregated_offers, complex_dir)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∫–≤–∞—Ä—Ç–∏—Ä: {e}")
                    processed_apartment_types = aggregated_offers

            # –ü–æ—Å–ª–µ —Å–±–æ—Ä–∞ –≤—Å–µ—Ö –æ—Ñ—Ñ–µ—Ä–æ–≤: –µ—Å–ª–∏ –µ—Å—Ç—å hod_url ‚Äî –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∏ —Å–æ–±–∏—Ä–∞–µ–º —Ö–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞.
            # –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö (–ø—Ä–æ–∫—Å–∏/—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ) ‚Äî –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –∏ –ø—Ä–æ–±—É–µ–º –µ—â—ë —Ä–∞–∑.
            if aggregated_hod_url:
                complex_id = get_complex_id_from_url(aggregated_complex_href or base_url)
                complex_dir = create_complex_directory(complex_id)
                max_attempts_hod = 3
                attempt_hod = 0
                while attempt_hod < max_attempts_hod and not construction_progress_data:
                    attempt_hod += 1
                    try:
                        print(f"  –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞: {aggregated_hod_url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt_hod}/{max_attempts_hod})")
                        stages_data = await extract_construction_from_domclick(page, aggregated_hod_url)
                        if stages_data and stages_data.get('construction_stages'):
                            print(f"  –ù–∞–π–¥–µ–Ω–æ —ç—Ç–∞–ø–æ–≤: {len(stages_data['construction_stages'])}")
                            construction_progress_data = await process_construction_stages_domclick(stages_data['construction_stages'], complex_dir)
                            break
                        else:
                            print("  ‚ö†Ô∏è –≠—Ç–∞–ø—ã –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞")
                            # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –±—Ä–∞—É–∑–µ—Ä –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –ø–æ–ø—ã—Ç–∫—É
                            if attempt_hod < max_attempts_hod:
                                try:
                                    browser, page, _ = await restart_browser(browser, headless=False)
                                except Exception:
                                    pass
                    except Exception as e:
                        print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞: {e}")
                        if attempt_hod < max_attempts_hod:
                            try:
                                browser, page, _ = await restart_browser(browser, headless=False)
                                print("  üîÑ –ë—Ä–∞—É–∑–µ—Ä –ø–µ—Ä–µ–∑–∞–ø—É—â–µ–Ω –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏ —Ö–æ–¥–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞")
                            except Exception as restart_error:
                                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞: {restart_error}")

            db_item = to_db_item(complex_photos_paths, processed_apartment_types)
            if construction_progress_data:
                db_item.setdefault('development', {})['construction_progress'] = construction_progress_data

            try:
                save_to_mongodb([db_item])


            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ MongoDB: {e}. –°–æ—Ö—Ä–∞–Ω—é –≤ {str(OUTPUT_FILE)} –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.")
                results.append({"sourceUrl": base_url,
                                "data": {"address": aggregated_address, "complexName": aggregated_complex_name,
                                         "complexHref": aggregated_complex_href, "offers": processed_apartment_types,
                                         "complexPhotosPaths": complex_photos_paths}})
                with open(str(OUTPUT_FILE), 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)

            url_index += 1
            offset = 0
            save_progress(url_index, offset, str(PROGRESS_FILE))
    finally:
        try:
            await browser.close()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –∑–∞–∫—Ä—ã—Ç–∏—è –±—Ä–∞—É–∑–µ—Ä–∞


if __name__ == "__main__":
    asyncio.get_event_loop().run_until_complete(run())
